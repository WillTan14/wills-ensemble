{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Dataset\n",
    "\n",
    "Code below is from https://www.kaggle.com/code/eitharyassin/fake-news-detection-on-pheme-newfeatures  \n",
    "Will improve/clean up later, but an alright data preparation for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow as tf\\ntfk = tf.keras\\ntfkl = tf.keras.layers'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import json\n",
    "#from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, LSTM, GRU, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\"\"\"\n",
    "#from sklearn import metrics\n",
    "\"\"\"from keras.initializers import Constant\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "import nltk # NLP  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer \n",
    "import re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\OneDrive\\Documents\\uni 2023\\GENG5511 MPE Project\\wills ensemble\\PHEME_veracity\\all-rnr-annotated-threads\n"
     ]
    }
   ],
   "source": [
    "%cd PHEME_veracity\\all-rnr-annotated-threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed initialization\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "#tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the folders to iterate on\n",
    "folds= ['charliehebdo-all-rnr-threads','ottawashooting-all-rnr-threads',\n",
    "'ebola-essien-all-rnr-threads','prince-toronto-all-rnr-threads',\n",
    "'ferguson-all-rnr-threads',\t'putinmissing-all-rnr-threads',\n",
    "'germanwings-crash-all-rnr-threads',\n",
    "'gurlitt-all-rnr-threads','sydneysiege-all-rnr-threads']\n",
    "\n",
    "\n",
    "texts = []\n",
    "fav_counts = []\n",
    "retweet_counts = []\n",
    "date = []\n",
    "\n",
    "username = []\n",
    "account_date = []\n",
    "protected = []\n",
    "verified = []\n",
    "followers = []\n",
    "followings = []\n",
    "tweets_count = []\n",
    "\n",
    "hashtag = []\n",
    "url = []\n",
    "\n",
    "events = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the annotations on PHEME have 2 boolean values, 'misinformation' value and 'true' value, we want to convert them in a single label with values\n",
    "# 'true', 'false' or 'unverified'. This values can be string or numerical, depending on the 'string' parameter of the function\n",
    "def convert_annotations_data(annotation, string = True):\n",
    "    if 'misinformation' in annotation.keys() and 'true'in annotation.keys():\n",
    "        if int(annotation['misinformation'])==0 and int(annotation['true'])==0:\n",
    "            if string:\n",
    "                label = \"unverified\"\n",
    "            else:\n",
    "                label = 2\n",
    "        elif int(annotation['misinformation'])==0 and int(annotation['true'])==1 :\n",
    "            if string:\n",
    "                label = \"true\"\n",
    "            else:\n",
    "                label = 1\n",
    "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==0 :\n",
    "            if string:\n",
    "                label = \"false\"\n",
    "            else:\n",
    "                label = 0\n",
    "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==1:\n",
    "            print (\"OMG! They both are 1!\")\n",
    "            print(annotation['misinformation'])\n",
    "            print(annotation['true'])\n",
    "            label = None\n",
    "            \n",
    "    elif 'misinformation' in annotation.keys() and 'true' not in annotation.keys():\n",
    "        # all instances have misinfo label but don't have true label\n",
    "        if int(annotation['misinformation'])==0:\n",
    "            if string:\n",
    "                label = \"unverified\"\n",
    "            else:\n",
    "                label = 2\n",
    "        elif int(annotation['misinformation'])==1:\n",
    "            if string:\n",
    "                label = \"false\"\n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "    elif 'true' in annotation.keys() and 'misinformation' not in annotation.keys():\n",
    "        print ('Has true not misinformation')\n",
    "        label = None\n",
    "    else:\n",
    "        print('No annotations')\n",
    "        label = None\n",
    "           \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all interesting features\n",
    "for f in folds:\n",
    "  path1 = os.path.join(f, 'rumours')\n",
    "  for dir1 in os.listdir(path1):\n",
    "        if '_' not in dir1:\n",
    "          path_target  = os.path.join(path1,dir1,'annotation.json')\n",
    "          file = open(path_target)\n",
    "          data = json.load(file)\n",
    "          target = convert_annotations_data(data)\n",
    "          y.append(target)\n",
    "          path2 = os.path.join(path1, dir1,'source-tweets')\n",
    "          for dir2 in os.listdir(path2):\n",
    "            if '_' not in dir2:\n",
    "              path3  = os.path.join(path2,dir2)\n",
    "              file = open(path3)\n",
    "              data = json.load(file)\n",
    "            \n",
    "              #tweet features\n",
    "              text = data['text']\n",
    "              tweet_date = data['created_at']\n",
    "              fav = data['favorite_count']\n",
    "              retw = data['retweet_count']\n",
    "                \n",
    "              #user features\n",
    "              usernames = data['user']['screen_name']\n",
    "              account_creation = data['user']['created_at']\n",
    "              is_protected = data['user']['protected']\n",
    "              is_verified = data['user']['verified']\n",
    "              no_followers = data['user']['followers_count']\n",
    "              no_followings = data['user']['friends_count']\n",
    "              no_tweets = data['user']['statuses_count']\n",
    "                \n",
    "              #entities\n",
    "              no_hashtags = len(data['entities']['hashtags'])      \n",
    "              has_url = data['entities']['urls']      \n",
    "   \n",
    "\n",
    "              texts.append(text)\n",
    "              date.append(tweet_date)\n",
    "              fav_counts.append(fav)\n",
    "              retweet_counts.append(retw)\n",
    "                                     \n",
    "              username.append(usernames)\n",
    "              account_date.append(account_creation)\n",
    "              protected.append(is_protected)\n",
    "              verified.append(is_verified)\n",
    "              followers.append(no_followers)\n",
    "              followings.append(no_followings)\n",
    "              tweets_count.append(no_tweets)\n",
    "            \n",
    "              \n",
    "              hashtag.append(no_hashtags)\n",
    "              url.append(has_url)\n",
    "            \n",
    "              events.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unverified'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([texts,date,fav_counts,retweet_counts,username,account_date,followers,followings,tweets_count,protected,verified,hashtag,url,events,y],['text','date','fav_count','retweet_count','username','account_date','followers','followings','tweet_count','protected','verified','no_hashtags','urls','event','target']).transpose()\n",
    "df = df.infer_objects()\n",
    "df.target[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\willc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\willc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk stuff\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_characters(text):\n",
    "    return re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "import contractions\n",
    "def remove_contractions(text):\n",
    "    return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in nltk.word_tokenize(text) if word not in stop_words])\n",
    "# str(text).split()\n",
    "\n",
    "#defining the object for Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#defining a function for lemming\n",
    "def lemmatize_words(text):\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "#defining the object for stemming\n",
    "stemmer = PorterStemmer()\n",
    "#defining a function for stemming\n",
    "def stemming_words(text):\n",
    "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_url(text)\n",
    "    text = remove_contractions(text)\n",
    "    text = text.lower()\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    #text = stemming_words(text)\n",
    "    #text = lemmatize_words(text)\n",
    "    return text\n",
    "    \n",
    "# apply\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data / Encode text\n",
    "\n",
    "# Remove unverified\n",
    "df = df.drop(df[df.target == 'unverified'].index)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_tf = tfidf.fit_transform(df['text'].values)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select encoding\n",
    "X = X_tf\n",
    "#X = X_cv\n",
    "y = df['target']\n",
    "\n",
    "rs = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = rs)\n",
    "\n",
    "# Set no. of folds\n",
    "num_folds = 10\n",
    "\n",
    "###Things to do: add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "[CV 1/10] END ..................C=0.1, gamma=10;, score=0.617 total time=   0.0s\n",
      "[CV 2/10] END ..................C=0.1, gamma=10;, score=0.617 total time=   0.0s\n",
      "[CV 3/10] END ..................C=0.1, gamma=10;, score=0.617 total time=   0.0s\n",
      "[CV 4/10] END ..................C=0.1, gamma=10;, score=0.609 total time=   0.0s\n",
      "[CV 5/10] END ..................C=0.1, gamma=10;, score=0.609 total time=   0.0s\n",
      "[CV 6/10] END ..................C=0.1, gamma=10;, score=0.609 total time=   0.0s\n",
      "[CV 7/10] END ..................C=0.1, gamma=10;, score=0.609 total time=   0.0s\n",
      "[CV 8/10] END ..................C=0.1, gamma=10;, score=0.609 total time=   0.0s\n",
      "[CV 9/10] END ..................C=0.1, gamma=10;, score=0.614 total time=   0.0s\n",
      "[CV 10/10] END .................C=0.1, gamma=10;, score=0.614 total time=   0.0s\n",
      "[CV 1/10] END ...................C=0.1, gamma=1;, score=0.672 total time=   0.0s\n",
      "[CV 2/10] END ...................C=0.1, gamma=1;, score=0.656 total time=   0.0s\n",
      "[CV 3/10] END ...................C=0.1, gamma=1;, score=0.680 total time=   0.0s\n",
      "[CV 4/10] END ...................C=0.1, gamma=1;, score=0.656 total time=   0.0s\n",
      "[CV 5/10] END ...................C=0.1, gamma=1;, score=0.648 total time=   0.0s\n",
      "[CV 6/10] END ...................C=0.1, gamma=1;, score=0.664 total time=   0.0s\n",
      "[CV 7/10] END ...................C=0.1, gamma=1;, score=0.672 total time=   0.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Set verbose = 3 here for more info              \u001b[39;00m\n\u001b[0;32m      9\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(SVC(random_state \u001b[39m=\u001b[39m rs), param_grid, refit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cv\u001b[39m=\u001b[39mnum_folds, verbose \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe best parameters for SVM are: \u001b[39m\u001b[39m{\u001b[39;00mgrid\u001b[39m.\u001b[39mbest_params_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Run model with best results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\svm\\_base.py:252\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    251\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m--> 252\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m    253\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\sklearn\\svm\\_base.py:373\u001b[0m, in \u001b[0;36mBaseLibSVM._sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    359\u001b[0m kernel_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_kernels\u001b[39m.\u001b[39mindex(kernel)\n\u001b[0;32m    361\u001b[0m libsvm_sparse\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    363\u001b[0m (\n\u001b[0;32m    364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[0;32m    366\u001b[0m     dual_coef_data,\n\u001b[0;32m    367\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[0;32m    369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[0;32m    371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[0;32m    372\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_iter,\n\u001b[1;32m--> 373\u001b[0m ) \u001b[39m=\u001b[39m libsvm_sparse\u001b[39m.\u001b[39;49mlibsvm_sparse_train(\n\u001b[0;32m    374\u001b[0m     X\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m],\n\u001b[0;32m    375\u001b[0m     X\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    376\u001b[0m     X\u001b[39m.\u001b[39;49mindices,\n\u001b[0;32m    377\u001b[0m     X\u001b[39m.\u001b[39;49mindptr,\n\u001b[0;32m    378\u001b[0m     y,\n\u001b[0;32m    379\u001b[0m     solver_type,\n\u001b[0;32m    380\u001b[0m     kernel_type,\n\u001b[0;32m    381\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    383\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    384\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[0;32m    386\u001b[0m     \u001b[39m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    387\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_class_weight\u001b[39;49m\u001b[39m\"\u001b[39;49m, np\u001b[39m.\u001b[39;49mempty(\u001b[39m0\u001b[39;49m)),\n\u001b[0;32m    388\u001b[0m     sample_weight,\n\u001b[0;32m    389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[0;32m    390\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    391\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[0;32m    392\u001b[0m     \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking),\n\u001b[0;32m    393\u001b[0m     \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability),\n\u001b[0;32m    394\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    395\u001b[0m     random_seed,\n\u001b[0;32m    396\u001b[0m )\n\u001b[0;32m    398\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n\u001b[0;32m    400\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclasses_\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32msklearn\\svm\\_libsvm_sparse.pyx:191\u001b[0m, in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\willc\\miniconda3\\envs\\test2\\lib\\site-packages\\scipy\\sparse\\_compressed.py:26\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_cs_matrix\u001b[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001b[0;32m     24\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"base matrix class for compressed row- and column-oriented matrices\"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, arg1, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     27\u001b[0m         _data_matrix\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m     29\u001b[0m         \u001b[39mif\u001b[39;00m isspmatrix(arg1):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tune hyperparameters\n",
    "param_grid = {'C': [10**x for x in range(-1, 8, 1)], \n",
    "              'gamma': [10**x for x in range(1, -8, -1)]} \n",
    "# Set verbose = 3 here for more info              \n",
    "grid = GridSearchCV(SVC(random_state = rs), param_grid, refit=True, cv=num_folds, verbose = 3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"The best parameters for SVM are: {grid.best_params_}\")\n",
    "\n",
    "# Run model with best results\n",
    "svm = grid.best_estimator_\n",
    "svm.fit(X_train, y_train)\n",
    "pred_y_svm = svm.predict(X_test)\n",
    "acc_svm = accuracy_score(y_test, pred_y_svm)\n",
    "print(\"Accuracy:\", float(\"{0:.2f}\".format(acc_svm*100)), \"%\")\n",
    "cm_svm = confusion_matrix(y_test, pred_y_svm)\n",
    "print(cm_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\willc\\miniconda3\\envs\\test\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\willc\\miniconda3\\envs\\test ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Tune hyperparameters\n",
    "param_grid = {'n_neighbors': np.arange(2, 15, 1)} \n",
    "# Set verbose = 3 here for more info  \n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit=True, cv=num_folds, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"The best parameters for KNN are: {grid.best_params_}\")\n",
    "\n",
    "# Run model\n",
    "knn = grid.best_estimator_\n",
    "knn.fit(X_train, y_train)\n",
    "pred_y_knn = knn.predict(X_test)\n",
    "acc_knn = accuracy_score(y_test, pred_y_svm)\n",
    "print(\"Accuracy:\", float(\"{0:.2f}\".format(acc_svm*100)), \"%\")\n",
    "cm_knn = confusion_matrix(y_test, pred_y_knn)\n",
    "print(cm_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\willc\\miniconda3\\envs\\test\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\willc\\miniconda3\\envs\\test ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Tune hyperparameters\n",
    "param_grid = {'C': [10**x for x in range(-1, 8, 1)]} \n",
    "# Set verbose = 3 here for more info  \n",
    "grid = GridSearchCV(LogisticRegression(solver = 'lbfgs', max_iter=10000), param_grid, refit=True, cv=num_folds, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"The best parameters for Logistic Regression are: {grid.best_params_}\")\n",
    "\n",
    "lr = grid.best_estimator_\n",
    "lr.fit(X_train, y_train)\n",
    "pred_y_lr = lr.predict(X_test)\n",
    "acc_lr = accuracy_score(y_test, pred_y_lr)\n",
    "print(\"Accuracy:\", float(\"{0:.2f}\".format(acc_lr*100)), \"%\")\n",
    "cm_lr = confusion_matrix(y_test, pred_y_lr)\n",
    "print(cm_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\willc\\miniconda3\\envs\\test\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\willc\\miniconda3\\envs\\test ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Tune hyperparameters\n",
    "param_grid = {'learning_rate': [10**x for x in range(1,-6, -1)],\n",
    "              'n_estimators': [10, 50, 100, 250, 500]} \n",
    "# Set verbose = 3 here for more info  \n",
    "grid = GridSearchCV(AdaBoostClassifier(n_estimators = 200, random_state = rs), param_grid, refit=True, cv=num_folds, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"The best parameters for AdaBoost are: {grid.best_params_}\")\n",
    "\n",
    "# Run model\n",
    "ada = grid.best_estimator_\n",
    "ada.fit(X_train, y_train)\n",
    "pred_y_ada = ada.predict(X_test)\n",
    "acc_ada = accuracy_score(y_test, pred_y_ada)\n",
    "print(\"Accuracy:\", float(\"{0:.2f}\".format(acc_ada*100)), \"%\")\n",
    "cm_ada = confusion_matrix(y_test, pred_y_ada)\n",
    "print(cm_ada)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
