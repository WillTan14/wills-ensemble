{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize models\n",
    "Run this notebook to optimize a set of models. Models will be saved in the \"optimized_models.txt\" file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial dataset stuff\n",
    "nlp = spacy.load(\"spacy-twitter\") # out of function so you don't load it every time (it takes a while)\n",
    "\n",
    "# function for glove embeddings\n",
    "def embed_dataset(dataset_text):\n",
    "    encoded = np.array([nlp(text).vector for text in dataset_text])\n",
    "    return encoded.tolist()\n",
    "\n",
    "# function to load dataset from folder. Also embeds the text.\n",
    "def get_dataset(name):\n",
    "    \"\"\"\n",
    "    loads a dataset and embeds the text. text must be in column named \"text\".\n",
    "    datasets are in the folder datasets/\n",
    "    name must be a string that's matches the csv file in datasets\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(f'datasets/{name}.csv')\n",
    "    dataset.rename(columns = {\"Unnamed: 0\":\"entry\"}, inplace=True) #the entry label never carries over\n",
    "    dataset['e_text'] = embed_dataset(dataset['text'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_parameters(search_space, objective, evals):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn = objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=evals,\n",
    "        timeout=120,\n",
    "        trials=trials,\n",
    "        verbose=False\n",
    "    )\n",
    "    set_params = space_eval(search_space, best_params)\n",
    "    score = trials.best_trial['result']['loss']\n",
    "    return set_params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_models(dataset_name, train_set, confidence, size_limit, model_list):\n",
    "    \"\"\"\n",
    "    Optimizes a set of models in each category. returns the best model for each category, in the form {'category': [modelscore, modelname, fittedmodel]}\n",
    "\n",
    "    dataset_name: a string with the name of the training set. used for calling the category file\n",
    "    train_set: the training set to use\n",
    "    confidence: the confidence required to consider an entry part of a category\n",
    "    size_limit: the number of entries needed in a category to consider that category for training\n",
    "    model_list: the list of models to train. in the form [(\"model_name1\", model1), etc]\n",
    "    \"\"\"\n",
    "    file_name = f\"{dataset_name}_cats/{dataset_name}_categories_organised.json\"\n",
    "    f = open(file_name)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    category_models = {} #this will be returned\n",
    "    all_models = {}\n",
    "    for category in data.keys(): \n",
    "        cat_entries = [int(i) for i in data[category].keys() if data[category][i] > confidence]\n",
    "        \n",
    "        # skip category if size of category is below limit\n",
    "        if len(cat_entries) < size_limit:\n",
    "            print(f\"Skipped category: {category} due to low numbers\")\n",
    "            continue\n",
    "        \n",
    "        category_data = train_set.filter(axis=0, items=cat_entries)\n",
    "\n",
    "        #split validation set\n",
    "        X = category_data.drop('target', axis=1)\n",
    "        y = category_data[\"target\"]\n",
    "        try:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "        except:\n",
    "            print(f\"Skipped category: {category} due to class issues\")\n",
    "            continue\n",
    "\n",
    "        # skip category if split only has one class\n",
    "        if (len(np.unique(y_train)) <= 1):\n",
    "            print(f\"Skipped category: {category} due to class issues\")\n",
    "            continue\n",
    "\n",
    "        X_train_text = np.array([text for text in X_train['e_text']])\n",
    "        X_val_text = np.array([text for text in X_val['e_text']])\n",
    "\n",
    "        trained_models = []\n",
    "        all_models[category] = []\n",
    "        # train models from list\n",
    "        for model_name, search_space, mod in model_list:\n",
    "            def objective(search_space):\n",
    "                warnings.filterwarnings('ignore')\n",
    "                model = mod.set_params(**search_space)\n",
    "                model.fit(X_train_text, y_train)\n",
    "                y_pred = model.predict(X_val_text)\n",
    "                accuracy = accuracy_score(y_val, y_pred)\n",
    "                return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "            try:\n",
    "                best_params, score = get_best_parameters(search_space, objective, 200)\n",
    "                mod.set_params(**best_params)\n",
    "                score *= -1\n",
    "                trained_models.append((model_name, mod))\n",
    "                all_models[category].append((model_name, mod))\n",
    "                #print(f\"Trained {model_name} on {category}\")\n",
    "            except:\n",
    "                print(f\"Error training {model_name} in category {category}, skipping\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "        #get the best model\n",
    "        best_model = [0, \"x\", \"x\"]\n",
    "        for name, model in trained_models:\n",
    "            score = model.score(X_val_text, y_val)\n",
    "            if score > best_model[0]:\n",
    "                best_model = [score, name, model]\n",
    "        \n",
    "        print(f\"Trained models on {category}, added {best_model[1]} to list\")\n",
    "        #add best model to list\n",
    "        category_models[category] = best_model\n",
    "    return category_models, all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_baseline(train_set, model_list):\n",
    "    X = train_set.drop('target', axis=1)\n",
    "    y = train_set[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.75, random_state=42, stratify=y_train)\n",
    "    X_train_text = np.array([text for text in X_train['e_text']])\n",
    "    X_val_text = np.array([text for text in X_val['e_text']])\n",
    "    trained_models = []\n",
    "    for model_name, search_space, mod in model_list:\n",
    "        def objective(search_space):\n",
    "            warnings.filterwarnings('ignore')\n",
    "            model = mod.set_params(**search_space)\n",
    "            model.fit(X_train_text, y_train)\n",
    "            y_pred = model.predict(X_val_text)\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "        try:\n",
    "            best_params, score = get_best_parameters(search_space, objective, 200)\n",
    "            mod.set_params(**best_params)\n",
    "            score *= -1\n",
    "            trained_models.append((model_name, mod))\n",
    "            #print(f\"Trained {model_name} on {category}\")\n",
    "        except:\n",
    "            print(f\"Error training {model_name}, skipping\")\n",
    "            continue\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare models and search spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_search_space={  \n",
    "                'C': hp.lognormal('C', 0, 1),\n",
    "                'kernel':hp.choice('kernel', [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "                'coef0':hp.uniform('coef0', 0.0, 1.0),\n",
    "                'shrinking':hp.choice('shrinking', [True, False]),\n",
    "                'tol':hp.loguniform('tol', np.log(1e-5), np.log(1e-2)),\n",
    "                'degree':hp.choice('degree', [1, 2, 3, 4, 5]),\n",
    "                'gamma':hp.choice('gamma', [\"scale\", \"auto\"]),\n",
    "                }\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_search_space={\n",
    "                \"n_neighbors\":hp.choice('n_neighbors', np.arange(1, 16, dtype=int)),\n",
    "                \"algorithm\":hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "                \"metric\": hp.choice(\"metric\", [\"cityblock\", \"l1\", \"l2\", \"minkowski\", \"euclidean\", \"manhattan\"]),\n",
    "                \"p\":hp.uniform(\"p\", 1, 5)\n",
    "                }\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "LR_search_space={\n",
    "                'C': hp.lognormal('C', 0, 1),\n",
    "                'penalty':hp.choice('p_saga',['elasticnet','l1','l2',None]),\n",
    "                'tol': hp.loguniform('tol',-13,-1),\n",
    "                'l1_ratio': hp.uniform('l1_ratio',0,1)\n",
    "                }\n",
    "\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_search_space={  'n_estimators':hp.randint('n_estimators',200,1000),\n",
    "                'max_depth': hp.randint('max_depth',10,200),                      \n",
    "                'min_samples_split':hp.uniform('min_samples_split',0,1),   \n",
    "                'min_samples_leaf':hp.randint('min_samples_leaf',1,10),            \n",
    "                'criterion':hp.choice('criterion',['gini','entropy']),               \n",
    "                'max_features':hp.choice('max_features',['sqrt', 'log2']) }\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_search_space={\n",
    "                'activation':hp.choice('activation', [\"identity\",\"logistic\",\"tanh\",\"relu\"]),\n",
    "                'solver':hp.choice('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "                'alpha':hp.uniform(\"alpha\", 1e-4, 0.01),\n",
    "                'learning_rate':hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "                'learning_rate_init':hp.uniform(\"learning_rate_init\", 1e-4, 0.1),\n",
    "                'power_t':hp.uniform('power_t', 0.1, 0.9),\n",
    "                'tol':hp.uniform('tol', 1e-4, 0.01),\n",
    "                'momentum':hp.uniform('momentum', 0.8, 1.0),\n",
    "                'early_stopping':hp.choice('early_stopping', [True, False]),\n",
    "                'beta_1':hp.uniform(\"beta_1\", 0.8, 1.0),\n",
    "                'beta_2':hp.uniform(\"beta_2\", 0.95, 1.0),\n",
    "                'epsilon':hp.uniform(\"epsilon\", 1e-9, 1e-5)\n",
    "                }\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_search_space={\n",
    "                'var_smoothing': 10**-9\n",
    "                }\n",
    "\n",
    "# SGD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGD_search_space={\n",
    "                'loss':hp.choice('loss',[\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]),\n",
    "                'penalty':hp.choice(\"penalty\", [\"l2\", \"l1\", \"elasticnet\", None]),\n",
    "                'alpha':hp.loguniform(\"alpha\", np.log(1e-6), np.log(1e-1)),\n",
    "                \"l1_ratio\":hp.loguniform(\"l1_ratio\", np.log(1e-7), np.log(1)),\n",
    "                \"tol\":hp.loguniform(\"tol\", np.log(1e-5), np.log(1e-2)),\n",
    "                'learning_rate':hp.choice(\"learning_rate\",  [\"optimal\", \"invscaling\", \"constant\", \"adaptive\"]),\n",
    "                'eta0':hp.loguniform(\"eta0\", np.log(1e-5), np.log(1e-1))\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_v2 = [\n",
    "    (\"SVM\", SVM_search_space, SVC(random_state=42)),\n",
    "    (\"KNN\", KNN_search_space, KNeighborsClassifier(n_jobs=-1)),\n",
    "    (\"Logistic Regression\", LR_search_space, LogisticRegression(solver=\"saga\", max_iter=1000, random_state=42, n_jobs=-1)),\n",
    "    (\"Random Forest\", RF_search_space, RandomForestClassifier()),\n",
    "    (\"MLP\", MLP_search_space, MLPClassifier()),\n",
    "    (\"Gaussian NB\", NB_search_space, GaussianNB()),\n",
    "    (\"SGD\", SGD_search_space, SGDClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(dataset):\n",
    "    d = get_dataset(dataset)\n",
    "    X = d.drop('target', axis=1)\n",
    "    y = d['target']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "    train_d = pd.concat([X_train, y_train], axis=1)\n",
    "    val_d = pd.concat([X_val, y_val], axis=1)\n",
    "    return train_d, val_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_t, pheme_v = arrange_data(\"pheme\")\n",
    "twitter_t, twitter_v = arrange_data(\"twitter\")\n",
    "weibo_t, weibo_v = arrange_data(\"weibo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single optimization of configuration\n",
    "conf = 0.2\n",
    "size = 150\n",
    "dataset_name = \"pheme\"\n",
    "dataset = pheme_t\n",
    "models, all_models = optimize_models(dataset_name, dataset, conf, size, model_list_v2)\n",
    "with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "    for key, value in models.items():\n",
    "        f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization of a set of configurations (modifying size)\n",
    "size_list = [20, 50, 100, 150, 200]\n",
    "conf = 0.3\n",
    "dataset_name = \"pheme\"\n",
    "dataset = pheme_t\n",
    "\n",
    "for size in size_list:\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(f\"{dataset_name} Models with {conf} confidence and {size} size\\n\\n\")\n",
    "\n",
    "    models, all_models = optimize_models(dataset_name, dataset, conf, size, model_list_v2)\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(f\"{dataset_name}\")\n",
    "        for key, value in models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization of a set of configurations (modifying confidence)\n",
    "size = 50\n",
    "conf_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dataset_name = \"pheme\"\n",
    "dataset = pheme_t\n",
    "\n",
    "for conf in conf_list:\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(f\"{dataset_name} Models with {conf} confidence and {size} size\\n\\n\")\n",
    "\n",
    "    models, all_models = optimize_models(dataset_name, dataset, conf, size, model_list_v2)\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(f\"{dataset_name}\")\n",
    "        for key, value in models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
