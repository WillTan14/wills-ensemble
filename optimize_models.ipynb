{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial dataset stuff\n",
    "nlp = spacy.load(\"spacy-twitter\") # out of function so you don't load it every time (it takes a while)\n",
    "\n",
    "# function for glove embeddings\n",
    "def embed_dataset(dataset_text):\n",
    "    encoded = np.array([nlp(text).vector for text in dataset_text])\n",
    "    return encoded.tolist()\n",
    "\n",
    "# function to load dataset from folder. Also embeds the text.\n",
    "def get_dataset(name):\n",
    "    \"\"\"\n",
    "    loads a dataset and embeds the text. text must be in column named \"text\".\n",
    "    datasets are in the folder datasets/\n",
    "    name must be a string that's matches the csv file in datasets\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(f'datasets/{name}.csv')\n",
    "    dataset.rename(columns = {\"Unnamed: 0\":\"entry\"}, inplace=True) #the entry label never carries over\n",
    "    dataset['e_text'] = embed_dataset(dataset['text'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred_y = model.predict(X_test)\n",
    "    acc_mod = accuracy_score(y_test, pred_y)\n",
    "    print(\"Accuracy:\", float(\"{0:.2f}\".format(acc_mod*100)), \"%\")\n",
    "    f1_mod = f1_score(y_test, pred_y, average=\"macro\")\n",
    "    print(\"F1:\", float(\"{0:.2f}\".format(f1_mod*100)), \"%\")\n",
    "    cm = confusion_matrix(y_test, pred_y)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"true\", \"false\"])\n",
    "    disp.plot()\n",
    "    plt.show() \n",
    "    \n",
    "def optimize_model_v2(search_space, objective, evals):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn = objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=evals,\n",
    "        timeout=120,\n",
    "        trials=trials\n",
    "    )\n",
    "    set_params = space_eval(search_space, best_params)\n",
    "    score = trials.best_trial['result']['loss']\n",
    "    return set_params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(dataset_name, train_set, confidence, size_limit, model_list):\n",
    "    \"\"\"\n",
    "    trains a set of models in each category. returns the best model for each category, in the form {'category': [modelscore, modelname, fittedmodel]}\n",
    "\n",
    "    dataset_name: a string with the name of the training set. used for calling the category file\n",
    "    train_set: the training set to use\n",
    "    confidence: the confidence required to consider an entry part of a category\n",
    "    size_limit: the number of entries needed in a category to consider that category for training\n",
    "    model_list: the list of models to train. in the form [(\"model_name1\", model1), etc]\n",
    "    \"\"\"\n",
    "    file_name = f\"{dataset_name}_cats/{dataset_name}_categories_organised.json\"\n",
    "    f = open(file_name)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    category_models = {} #this will be returned\n",
    "    for category in data.keys(): \n",
    "        cat_entries = [int(i) for i in data[category].keys() if data[category][i] > confidence]\n",
    "        \n",
    "        # skip category if size of category is below limit\n",
    "        if len(cat_entries) < size_limit:\n",
    "            print(f\"Skipped category: {category} due to low numbers\")\n",
    "            continue\n",
    "        \n",
    "        category_data = train_set.filter(axis=0, items=cat_entries)\n",
    "\n",
    "        #split validation set\n",
    "        X = category_data.drop('target', axis=1)\n",
    "        y = category_data[\"target\"]\n",
    "        try:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "        except:\n",
    "            print(f\"Skipped category: {category} due to class issues\")\n",
    "            continue\n",
    "\n",
    "        # skip category if split only has one class\n",
    "        if (len(np.unique(y_train)) <= 1):\n",
    "            print(f\"Skipped category: {category} due to class issues\")\n",
    "            continue\n",
    "\n",
    "        X_train_text = np.array([text for text in X_train['e_text']])\n",
    "        X_val_text = np.array([text for text in X_val['e_text']])\n",
    "\n",
    "        trained_models = []\n",
    "        # train models from list\n",
    "        for model_name, search_space, mod in model_list:\n",
    "            def objective(search_space):\n",
    "                warnings.filterwarnings('ignore')\n",
    "                model = mod.set_params(**search_space)\n",
    "                model.fit(X_train_text, y_train)\n",
    "                y_pred = model.predict(X_val_text)\n",
    "                accuracy = accuracy_score(y_val, y_pred)\n",
    "                return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "            try:\n",
    "                best_params, score = optimize_model_v2(search_space, objective, 200)\n",
    "                mod.set_params(**best_params)\n",
    "                score *= -1\n",
    "                trained_models.append((model_name, mod))\n",
    "                #print(f\"Trained {model_name} on {category}\")\n",
    "            except:\n",
    "                print(f\"Error training {model_name} in category {category}, skipping\")\n",
    "                continue\n",
    "            trained_models.append((model_name, mod))\n",
    "\n",
    "        #get the best model\n",
    "        best_model = [0, \"x\", \"x\"]\n",
    "        for name, model in trained_models:\n",
    "            score = model.score(X_val_text, y_val)\n",
    "            if score > best_model[0]:\n",
    "                best_model = [score, name, model]\n",
    "        \n",
    "        print(f\"Trained models on {category}, added {best_model[1]} to list\")\n",
    "        #add best model to list\n",
    "        category_models[category] = best_model\n",
    "    return category_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_search_space={  \n",
    "                'C': hp.lognormal('C', 0, 1),\n",
    "                'kernel':hp.choice('kernel', [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "                'coef0':hp.uniform('coef0', 0.0, 1.0),\n",
    "                'shrinking':hp.choice('shrinking', [True, False]),\n",
    "                'tol':hp.loguniform('tol', np.log(1e-5), np.log(1e-2)),\n",
    "                'degree':hp.choice('degree', [1, 2, 3, 4, 5]),\n",
    "                'gamma':hp.choice('gamma', [\"scale\", \"auto\"]),\n",
    "                }\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_search_space={\n",
    "                \"n_neighbors\":hp.choice('n_neighbors', np.arange(1, 16, dtype=int)),\n",
    "                \"algorithm\":hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "                \"metric\": hp.choice(\"metric\", [\"cityblock\", \"l1\", \"l2\", \"minkowski\", \"euclidean\", \"manhattan\"]),\n",
    "                \"p\":hp.uniform(\"p\", 1, 5)\n",
    "                }\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "LR_search_space={\n",
    "                'C': hp.lognormal('C', 0, 1),\n",
    "                'penalty':hp.choice('p_saga',['elasticnet','l1','l2',None]),\n",
    "                'tol': hp.loguniform('tol',-13,-1),\n",
    "                'l1_ratio': hp.uniform('l1_ratio',0,1)\n",
    "                }\n",
    "\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_search_space={  'n_estimators':hp.randint('n_estimators',200,1000),\n",
    "                'max_depth': hp.randint('max_depth',10,200),                      \n",
    "                'min_samples_split':hp.uniform('min_samples_split',0,1),   \n",
    "                'min_samples_leaf':hp.randint('min_samples_leaf',1,10),            \n",
    "                'criterion':hp.choice('criterion',['gini','entropy']),               \n",
    "                'max_features':hp.choice('max_features',['sqrt', 'log2']) }\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_search_space={\n",
    "                'activation':hp.choice('activation', [\"identity\",\"logistic\",\"tanh\",\"relu\"]),\n",
    "                'solver':hp.choice('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "                'alpha':hp.uniform(\"alpha\", 1e-4, 0.01),\n",
    "                'learning_rate':hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "                'learning_rate_init':hp.uniform(\"learning_rate_init\", 1e-4, 0.1),\n",
    "                'power_t':hp.uniform('power_t', 0.1, 0.9),\n",
    "                'tol':hp.uniform('tol', 1e-4, 0.01),\n",
    "                'momentum':hp.uniform('momentum', 0.8, 1.0),\n",
    "                'early_stopping':hp.choice('early_stopping', [True, False]),\n",
    "                'beta_1':hp.uniform(\"beta_1\", 0.8, 1.0),\n",
    "                'beta_2':hp.uniform(\"beta_2\", 0.95, 1.0),\n",
    "                'epsilon':hp.uniform(\"epsilon\", 1e-9, 1e-5)\n",
    "                }\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_search_space={\n",
    "                'var_smoothing': 10**-9\n",
    "                }\n",
    "\n",
    "# SGD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGD_search_space={\n",
    "                'loss':hp.choice('loss',[\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]),\n",
    "                'penalty':hp.choice(\"penalty\", [\"l2\", \"l1\", \"elasticnet\", None]),\n",
    "                'alpha':hp.loguniform(\"alpha\", np.log(1e-6), np.log(1e-1)),\n",
    "                \"l1_ratio\":hp.loguniform(\"l1_ratio\", np.log(1e-7), np.log(1)),\n",
    "                \"tol\":hp.loguniform(\"tol\", np.log(1e-5), np.log(1e-2)),\n",
    "                'learning_rate':hp.choice(\"learning_rate\",  [\"optimal\", \"invscaling\", \"constant\", \"adaptive\"]),\n",
    "                'eta0':hp.loguniform(\"eta0\", np.log(1e-5), np.log(1e-1))\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_v2 = [\n",
    "    (\"SVM\", SVM_search_space, SVC(random_state=42)),\n",
    "    (\"KNN\", KNN_search_space, KNeighborsClassifier(n_jobs=-1)),\n",
    "    (\"Logistic Regression\", LR_search_space, LogisticRegression(solver=\"saga\", max_iter=1000, random_state=42, n_jobs=-1)),\n",
    "    (\"Random Forest\", RF_search_space, RandomForestClassifier()),\n",
    "    (\"MLP\", MLP_search_space, MLPClassifier()),\n",
    "    (\"Gaussian NB\", NB_search_space, GaussianNB()),\n",
    "    (\"SGD\", SGD_search_space, SGDClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme = get_dataset(\"pheme\")\n",
    "X = pheme.drop('target', axis=1)\n",
    "y = pheme['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "pheme_train = pd.concat([X_train, y_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(dataset):\n",
    "    d = get_dataset(dataset)\n",
    "    X = d.drop('target', axis=1)\n",
    "    y = d['target']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "    train_d = pd.concat([X_train, y_train], axis=1)\n",
    "    val_d = pd.concat([X_val, y_val], axis=1)\n",
    "    return train_d, val_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_t, twitter_v = arrange_data(\"twitter\")\n",
    "weibo_t, weibo_v = arrange_data(\"weibo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39/200 [03:54<16:07,  6.01s/trial, best loss: -0.7597254004576659]  \n",
      "100%|██████████| 200/200 [00:40<00:00,  4.99trial/s, best loss: -0.7574370709382151]\n",
      "100%|██████████| 200/200 [01:20<00:00,  2.49trial/s, best loss: -0.7665903890160183]\n",
      " 18%|█▊        | 37/200 [02:00<08:50,  3.26s/trial, best loss: -0.7574370709382151]\n",
      "100%|██████████| 200/200 [01:08<00:00,  2.91trial/s, best loss: -0.7940503432494279]\n",
      "100%|██████████| 200/200 [00:01<00:00, 159.30trial/s, best loss: -0.7276887871853547]\n",
      "100%|██████████| 200/200 [01:09<00:00,  2.87trial/s, best loss: -0.7665903890160183]\n",
      "Trained models on Sensitive Subjects, added SGD to list\n",
      "100%|██████████| 200/200 [00:03<00:00, 54.89trial/s, best loss: -0.8035714285714286]\n",
      "100%|██████████| 200/200 [00:05<00:00, 33.69trial/s, best loss: -0.7857142857142857]\n",
      "100%|██████████| 200/200 [00:20<00:00,  9.73trial/s, best loss: -0.8214285714285714]\n",
      " 60%|██████    | 121/200 [02:00<01:18,  1.01trial/s, best loss: -0.75]             \n",
      "100%|██████████| 200/200 [00:29<00:00,  6.71trial/s, best loss: -0.8392857142857143]\n",
      "100%|██████████| 200/200 [00:00<00:00, 306.61trial/s, best loss: -0.75]\n",
      "100%|██████████| 200/200 [00:08<00:00, 22.68trial/s, best loss: -0.8214285714285714]\n",
      "Trained models on News, added KNN to list\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.12trial/s, best loss: -0.8947368421052632]\n",
      "100%|██████████| 200/200 [00:08<00:00, 24.95trial/s, best loss: -0.8947368421052632]\n",
      "100%|██████████| 200/200 [01:22<00:00,  2.44trial/s, best loss: -0.8771929824561403]\n",
      " 62%|██████▎   | 125/200 [02:00<01:12,  1.04trial/s, best loss: -0.8947368421052632]\n",
      "100%|██████████| 200/200 [00:20<00:00,  9.84trial/s, best loss: -0.9122807017543859]\n",
      "100%|██████████| 200/200 [00:00<00:00, 314.01trial/s, best loss: -0.8771929824561403]\n",
      "100%|██████████| 200/200 [00:07<00:00, 26.14trial/s, best loss: -0.9473684210526315]\n",
      "Trained models on Arts & Entertainment, added KNN to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 80.91trial/s, best loss: -1.0]\n",
      "100%|██████████| 200/200 [00:04<00:00, 41.37trial/s, best loss: -1.0]\n",
      "100%|██████████| 200/200 [00:24<00:00,  8.10trial/s, best loss: -1.0]             \n",
      " 38%|███▊      | 77/200 [01:04<01:42,  1.20trial/s, best loss: -0.9459459459459459]"
     ]
    }
   ],
   "source": [
    "models = train_models(\"pheme\", pheme_train, 0.5, 5, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Sensitive Subjects', 'News', 'Arts & Entertainment', 'People & Society', 'Law & Government', 'Online Communities', 'Travel & Transportation'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 55.15trial/s, best loss: -0.8727272727272727]\n",
      "100%|██████████| 200/200 [00:04<00:00, 42.63trial/s, best loss: -0.8727272727272727]\n",
      "100%|██████████| 200/200 [00:53<00:00,  3.76trial/s, best loss: -0.8181818181818182]\n",
      " 61%|██████    | 122/200 [02:00<01:16,  1.02trial/s, best loss: -0.8363636363636363]\n",
      "100%|██████████| 200/200 [00:23<00:00,  8.35trial/s, best loss: -0.8909090909090909]\n",
      "100%|██████████| 200/200 [00:00<00:00, 323.05trial/s, best loss: -0.7636363636363637]\n",
      "100%|██████████| 200/200 [00:08<00:00, 22.34trial/s, best loss: -0.8727272727272727]\n",
      "Trained models on People & Society, added KNN to list\n",
      "100%|██████████| 200/200 [00:06<00:00, 31.43trial/s, best loss: -0.8243243243243243]\n",
      "100%|██████████| 200/200 [00:06<00:00, 29.65trial/s, best loss: -0.7027027027027027]\n",
      "100%|██████████| 200/200 [00:59<00:00,  3.35trial/s, best loss: -0.7972972972972973]\n",
      " 50%|█████     | 101/200 [02:00<01:57,  1.19s/trial, best loss: -0.7432432432432432]\n",
      "100%|██████████| 200/200 [00:31<00:00,  6.33trial/s, best loss: -0.8648648648648649]\n",
      "100%|██████████| 200/200 [00:00<00:00, 294.64trial/s, best loss: -0.6351351351351351]\n",
      "100%|██████████| 200/200 [00:12<00:00, 15.89trial/s, best loss: -0.7972972972972973]\n",
      "Trained models on Arts & Entertainment, added Logistic Regression to list\n",
      "100%|██████████| 200/200 [00:04<00:00, 43.92trial/s, best loss: -0.8493150684931506]\n",
      "100%|██████████| 200/200 [00:06<00:00, 29.47trial/s, best loss: -0.821917808219178]\n",
      "100%|██████████| 200/200 [01:31<00:00,  2.20trial/s, best loss: -0.8082191780821918]\n",
      " 60%|██████    | 120/200 [02:00<01:20,  1.00s/trial, best loss: -0.7808219178082192]\n",
      "100%|██████████| 200/200 [00:30<00:00,  6.58trial/s, best loss: -0.8356164383561644]\n",
      "100%|██████████| 200/200 [00:00<00:00, 301.76trial/s, best loss: -0.6575342465753424]\n",
      "100%|██████████| 200/200 [00:11<00:00, 17.24trial/s, best loss: -0.7945205479452054]\n",
      "Trained models on Law & Government, added KNN to list\n",
      "100%|██████████| 200/200 [00:12<00:00, 15.48trial/s, best loss: -0.8378378378378378]\n",
      "100%|██████████| 200/200 [00:11<00:00, 17.44trial/s, best loss: -0.8378378378378378]\n",
      "100%|██████████| 200/200 [01:13<00:00,  2.70trial/s, best loss: -0.7905405405405406]\n",
      " 36%|███▋      | 73/200 [02:00<03:28,  1.65s/trial, best loss: -0.8175675675675675]\n",
      "100%|██████████| 200/200 [01:03<00:00,  3.14trial/s, best loss: -0.8445945945945946]\n",
      "100%|██████████| 200/200 [00:00<00:00, 264.39trial/s, best loss: -0.6959459459459459]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.87trial/s, best loss: -0.8040540540540541]\n",
      "Trained models on News, added KNN to list\n",
      "Skipped category: Food & Drink due to low numbers\n",
      "100%|██████████| 200/200 [00:11<00:00, 17.33trial/s, best loss: -0.7870967741935484]\n",
      "100%|██████████| 200/200 [00:13<00:00, 15.35trial/s, best loss: -0.7935483870967742]\n",
      "100%|██████████| 200/200 [01:53<00:00,  1.76trial/s, best loss: -0.8]              \n",
      " 28%|██▊       | 56/200 [02:00<05:08,  2.14s/trial, best loss: -0.7677419354838709]\n",
      "100%|██████████| 200/200 [00:49<00:00,  4.07trial/s, best loss: -0.8258064516129032]\n",
      "100%|██████████| 200/200 [00:00<00:00, 260.38trial/s, best loss: -0.6774193548387096]\n",
      "100%|██████████| 200/200 [00:17<00:00, 11.65trial/s, best loss: -0.8]              \n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "100%|██████████| 200/200 [00:03<00:00, 60.56trial/s, best loss: -0.7608695652173914]\n",
      "100%|██████████| 200/200 [00:06<00:00, 31.00trial/s, best loss: -0.8043478260869565]\n",
      "100%|██████████| 200/200 [00:20<00:00,  9.81trial/s, best loss: -0.7391304347826086]\n",
      " 64%|██████▍   | 129/200 [02:01<01:06,  1.07trial/s, best loss: -0.7608695652173914]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.79trial/s, best loss: -0.8478260869565217]\n",
      "100%|██████████| 200/200 [00:00<00:00, 318.25trial/s, best loss: -0.5869565217391305]\n",
      "100%|██████████| 200/200 [00:18<00:00, 11.09trial/s, best loss: -0.782608695652174] \n",
      "Trained models on Online Communities, added KNN to list\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Health due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Reference due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n",
      "Skipped category: Science due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Travel & Transportation due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n"
     ]
    }
   ],
   "source": [
    "twitter = get_dataset(\"twitter\")\n",
    "twitter_models = train_models(\"twitter\", twitter_t, 0.5, 5, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'People & Society': [0.8727272727272727,\n",
       "  'KNN',\n",
       "  KNeighborsClassifier(algorithm='ball_tree', metric='l2', n_jobs=-1,\n",
       "                       n_neighbors=4, p=4.010199404717891)],\n",
       " 'Arts & Entertainment': [0.7837837837837838,\n",
       "  'Logistic Regression',\n",
       "  LogisticRegression(C=1.1421137890725783, l1_ratio=0.49014451330123276,\n",
       "                     max_iter=1000, n_jobs=-1, random_state=42, solver='saga',\n",
       "                     tol=0.0013198619230799782)],\n",
       " 'Law & Government': [0.821917808219178,\n",
       "  'KNN',\n",
       "  KNeighborsClassifier(algorithm='ball_tree', metric='l2', n_jobs=-1,\n",
       "                       n_neighbors=4, p=4.010199404717891)],\n",
       " 'News': [0.8378378378378378,\n",
       "  'KNN',\n",
       "  KNeighborsClassifier(algorithm='ball_tree', metric='l2', n_jobs=-1,\n",
       "                       n_neighbors=4, p=4.010199404717891)],\n",
       " 'Sensitive Subjects': [0.7935483870967742,\n",
       "  'KNN',\n",
       "  KNeighborsClassifier(algorithm='ball_tree', metric='l2', n_jobs=-1,\n",
       "                       n_neighbors=4, p=4.010199404717891)],\n",
       " 'Online Communities': [0.8043478260869565,\n",
       "  'KNN',\n",
       "  KNeighborsClassifier(algorithm='ball_tree', metric='l2', n_jobs=-1,\n",
       "                       n_neighbors=4, p=4.010199404717891)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 20.29trial/s, best loss: -0.8029197080291971]\n",
      "100%|██████████| 200/200 [00:28<00:00,  7.06trial/s, best loss: -0.7883211678832117]\n",
      "100%|██████████| 200/200 [01:04<00:00,  3.12trial/s, best loss: -0.7956204379562044]\n",
      " 36%|███▌      | 71/200 [02:02<03:43,  1.73s/trial, best loss: -0.8321167883211679]\n",
      "100%|██████████| 200/200 [00:45<00:00,  4.40trial/s, best loss: -0.8248175182481752]\n",
      "100%|██████████| 200/200 [00:00<00:00, 268.59trial/s, best loss: -0.635036496350365]\n",
      "100%|██████████| 200/200 [00:12<00:00, 15.44trial/s, best loss: -0.7956204379562044]\n",
      "Trained models on Arts & Entertainment, added Random Forest to list\n",
      "100%|██████████| 200/200 [00:04<00:00, 42.34trial/s, best loss: -0.8953488372093024]\n",
      "100%|██████████| 200/200 [00:07<00:00, 26.43trial/s, best loss: -0.8837209302325582]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.92trial/s, best loss: -0.872093023255814]\n",
      " 42%|████▏     | 84/200 [02:00<02:46,  1.44s/trial, best loss: -0.8953488372093024]\n",
      "100%|██████████| 200/200 [00:55<00:00,  3.62trial/s, best loss: -0.8837209302325582]\n",
      "100%|██████████| 200/200 [00:00<00:00, 279.37trial/s, best loss: -0.7441860465116279]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.69trial/s, best loss: -0.872093023255814] \n",
      "Trained models on People & Society, added SVM to list\n",
      "Skipped category: Reference due to low numbers\n",
      "100%|██████████| 200/200 [00:03<00:00, 65.99trial/s, best loss: -0.8947368421052632]\n",
      "100%|██████████| 200/200 [00:05<00:00, 34.70trial/s, best loss: -0.9210526315789473]\n",
      "100%|██████████| 200/200 [00:31<00:00,  6.35trial/s, best loss: -0.8947368421052632]\n",
      " 62%|██████▎   | 125/200 [02:00<01:12,  1.03trial/s, best loss: -0.8421052631578947]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.97trial/s, best loss: -0.9473684210526315]\n",
      "100%|██████████| 200/200 [00:00<00:00, 310.33trial/s, best loss: -0.7894736842105263]\n",
      "100%|██████████| 200/200 [00:05<00:00, 36.45trial/s, best loss: -0.9210526315789473]\n",
      "Trained models on Food & Drink, added KNN to list\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Travel & Transportation due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 71.19trial/s, best loss: -0.8648648648648649]\n",
      "100%|██████████| 200/200 [00:04<00:00, 41.46trial/s, best loss: -0.8918918918918919]\n",
      "100%|██████████| 200/200 [00:30<00:00,  6.47trial/s, best loss: -0.8648648648648649]\n",
      " 63%|██████▎   | 126/200 [02:00<01:11,  1.04trial/s, best loss: -0.8378378378378378]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.37trial/s, best loss: -0.918918918918919] \n",
      "100%|██████████| 200/200 [00:00<00:00, 343.10trial/s, best loss: -0.8918918918918919]\n",
      "100%|██████████| 200/200 [00:06<00:00, 32.15trial/s, best loss: -0.9459459459459459]\n",
      "Trained models on Health, added MLP to list\n",
      "Skipped category: Online Communities due to low numbers\n",
      "100%|██████████| 200/200 [00:04<00:00, 40.59trial/s, best loss: -0.8539325842696629]\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.64trial/s, best loss: -0.7528089887640449]\n",
      "100%|██████████| 200/200 [01:17<00:00,  2.56trial/s, best loss: -0.8314606741573034]\n",
      " 38%|███▊      | 76/200 [02:01<03:18,  1.60s/trial, best loss: -0.797752808988764] \n",
      "100%|██████████| 200/200 [00:30<00:00,  6.56trial/s, best loss: -0.8651685393258427]\n",
      "100%|██████████| 200/200 [00:00<00:00, 286.58trial/s, best loss: -0.7303370786516854]\n",
      "100%|██████████| 200/200 [00:13<00:00, 15.31trial/s, best loss: -0.8651685393258427]\n",
      "Trained models on News, added MLP to list\n",
      "Skipped category: Science due to low numbers\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.06trial/s, best loss: -0.8768115942028986]\n",
      "100%|██████████| 200/200 [00:11<00:00, 16.72trial/s, best loss: -0.8840579710144928]\n",
      "100%|██████████| 200/200 [01:26<00:00,  2.31trial/s, best loss: -0.8840579710144928]\n",
      " 36%|███▋      | 73/200 [02:00<03:30,  1.65s/trial, best loss: -0.8115942028985508]\n",
      "100%|██████████| 200/200 [00:35<00:00,  5.59trial/s, best loss: -0.9130434782608695]\n",
      "100%|██████████| 200/200 [00:00<00:00, 264.94trial/s, best loss: -0.7898550724637681]\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.11trial/s, best loss: -0.8840579710144928]\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "100%|██████████| 200/200 [00:05<00:00, 39.97trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:06<00:00, 30.99trial/s, best loss: -0.7888888888888889]\n",
      "100%|██████████| 200/200 [00:34<00:00,  5.77trial/s, best loss: -0.8222222222222222]\n",
      " 44%|████▎     | 87/200 [02:00<02:36,  1.39s/trial, best loss: -0.8111111111111111]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.89trial/s, best loss: -0.8222222222222222]\n",
      "100%|██████████| 200/200 [00:00<00:00, 291.92trial/s, best loss: -0.7222222222222222]\n",
      "100%|██████████| 200/200 [00:11<00:00, 18.13trial/s, best loss: -0.8333333333333334]\n",
      "Trained models on Law & Government, added Random Forest to list\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n"
     ]
    }
   ],
   "source": [
    "weibo = get_dataset(\"weibo\")\n",
    "weibo_models = train_models(\"weibo\", weibo_t, 0.5, 5, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [models, twitter_models, weibo_models]\n",
    "for k in a:\n",
    "    with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        for key, value in k.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Arts & Entertainment\", \"SGD\", SGDClassifier(alpha=6.041815217486042e-05, eta0=0.03686744686987627,\n",
      "              l1_ratio=1.762040379810453e-05, learning_rate='constant',\n",
      "              loss='modified_huber', tol=0.006882240523359017)\n",
      "\n",
      "\"People & Society\", \"SVM\", SVC(C=23.186942440846998, coef0=0.893023010661618, degree=5, kernel='poly',\n",
      "    random_state=42, shrinking=False, tol=0.006378488923858316)\n",
      "\n",
      "\"Food & Drink\", \"KNN\", KNeighborsClassifier(metric='manhattan', n_jobs=-1, n_neighbors=9,\n",
      "                     p=1.75336550705507)\n",
      "\n",
      "\"Health\", \"SVM\", SVC(C=23.186942440846998, coef0=0.893023010661618, degree=5, kernel='poly',\n",
      "    random_state=42, shrinking=False, tol=0.006378488923858316)\n",
      "\n",
      "\"News\", \"Logistic Regression\", LogisticRegression(C=2.020080503767674, l1_ratio=0.5952769710777521,\n",
      "                   max_iter=1000, n_jobs=-1, random_state=42, solver='saga',\n",
      "                   tol=4.926500624596985e-06)\n",
      "\n",
      "\"Sensitive Subjects\", \"KNN\", KNeighborsClassifier(metric='manhattan', n_jobs=-1, n_neighbors=9,\n",
      "                     p=1.75336550705507)\n",
      "\n",
      "\"Law & Government\", \"Logistic Regression\", LogisticRegression(C=2.020080503767674, l1_ratio=0.5952769710777521,\n",
      "                   max_iter=1000, n_jobs=-1, random_state=42, solver='saga',\n",
      "                   tol=4.926500624596985e-06)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in k.items():\n",
    "    print(f\"\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumour-ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
