{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial dataset stuff\n",
    "nlp = spacy.load(\"spacy-twitter\") # out of function so you don't load it every time (it takes a while)\n",
    "\n",
    "# function for glove embeddings\n",
    "def embed_dataset(dataset_text):\n",
    "    encoded = np.array([nlp(text).vector for text in dataset_text])\n",
    "    return encoded.tolist()\n",
    "\n",
    "# function to load dataset from folder. Also embeds the text.\n",
    "def get_dataset(name):\n",
    "    \"\"\"\n",
    "    loads a dataset and embeds the text. text must be in column named \"text\".\n",
    "    datasets are in the folder datasets/\n",
    "    name must be a string that's matches the csv file in datasets\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(f'datasets/{name}.csv')\n",
    "    dataset.rename(columns = {\"Unnamed: 0\":\"entry\"}, inplace=True) #the entry label never carries over\n",
    "    dataset['e_text'] = embed_dataset(dataset['text'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred_y = model.predict(X_test)\n",
    "    acc_mod = accuracy_score(y_test, pred_y)\n",
    "    print(\"Accuracy:\", float(\"{0:.2f}\".format(acc_mod*100)), \"%\")\n",
    "    f1_mod = f1_score(y_test, pred_y, average=\"macro\")\n",
    "    print(\"F1:\", float(\"{0:.2f}\".format(f1_mod*100)), \"%\")\n",
    "    cm = confusion_matrix(y_test, pred_y)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"true\", \"false\"])\n",
    "    disp.plot()\n",
    "    plt.show() \n",
    "    \n",
    "def optimize_model_v2(search_space, objective, evals):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn = objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=evals,\n",
    "        timeout=120,\n",
    "        trials=trials,\n",
    "        verbose=False\n",
    "    )\n",
    "    set_params = space_eval(search_space, best_params)\n",
    "    score = trials.best_trial['result']['loss']\n",
    "    return set_params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(dataset_name, train_set, confidence, size_limit, model_list):\n",
    "    \"\"\"\n",
    "    trains a set of models in each category. returns the best model for each category, in the form {'category': [modelscore, modelname, fittedmodel]}\n",
    "\n",
    "    dataset_name: a string with the name of the training set. used for calling the category file\n",
    "    train_set: the training set to use\n",
    "    confidence: the confidence required to consider an entry part of a category\n",
    "    size_limit: the number of entries needed in a category to consider that category for training\n",
    "    model_list: the list of models to train. in the form [(\"model_name1\", model1), etc]\n",
    "    \"\"\"\n",
    "    file_name = f\"{dataset_name}_cats/{dataset_name}_categories_organised.json\"\n",
    "    f = open(file_name)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    category_models = {} #this will be returned\n",
    "    all_models = {}\n",
    "    for category in data.keys(): \n",
    "        cat_entries = [int(i) for i in data[category].keys() if data[category][i] > confidence]\n",
    "        \n",
    "        # skip category if size of category is below limit\n",
    "        if len(cat_entries) < size_limit:\n",
    "            print(f\"Skipped category: {category} due to low numbers\")\n",
    "            continue\n",
    "        \n",
    "        category_data = train_set.filter(axis=0, items=cat_entries)\n",
    "\n",
    "        #split validation set\n",
    "        X = category_data.drop('target', axis=1)\n",
    "        y = category_data[\"target\"]\n",
    "        try:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "        except:\n",
    "            print(f\"Skipped category: {category} due to class issues\")\n",
    "            continue\n",
    "\n",
    "        # skip category if split only has one class\n",
    "        if (len(np.unique(y_train)) <= 1):\n",
    "            print(f\"Skipped category: {category} due to class issues\")\n",
    "            continue\n",
    "\n",
    "        X_train_text = np.array([text for text in X_train['e_text']])\n",
    "        X_val_text = np.array([text for text in X_val['e_text']])\n",
    "\n",
    "        trained_models = []\n",
    "        all_models[category] = []\n",
    "        # train models from list\n",
    "        for model_name, search_space, mod in model_list:\n",
    "            def objective(search_space):\n",
    "                warnings.filterwarnings('ignore')\n",
    "                model = mod.set_params(**search_space)\n",
    "                model.fit(X_train_text, y_train)\n",
    "                y_pred = model.predict(X_val_text)\n",
    "                accuracy = accuracy_score(y_val, y_pred)\n",
    "                return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "            try:\n",
    "                best_params, score = optimize_model_v2(search_space, objective, 200)\n",
    "                mod.set_params(**best_params)\n",
    "                score *= -1\n",
    "                trained_models.append((model_name, mod))\n",
    "                all_models[category].append((model_name, mod))\n",
    "                #print(f\"Trained {model_name} on {category}\")\n",
    "            except:\n",
    "                print(f\"Error training {model_name} in category {category}, skipping\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "        #get the best model\n",
    "        best_model = [0, \"x\", \"x\"]\n",
    "        for name, model in trained_models:\n",
    "            score = model.score(X_val_text, y_val)\n",
    "            if score > best_model[0]:\n",
    "                best_model = [score, name, model]\n",
    "        \n",
    "        print(f\"Trained models on {category}, added {best_model[1]} to list\")\n",
    "        #add best model to list\n",
    "        category_models[category] = best_model\n",
    "    return category_models, all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(train_set, model_list):\n",
    "    X = train_set.drop('target', axis=1)\n",
    "    y = train_set[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.75, random_state=42, stratify=y_train)\n",
    "    X_train_text = np.array([text for text in X_train['e_text']])\n",
    "    X_val_text = np.array([text for text in X_val['e_text']])\n",
    "    trained_models = []\n",
    "    for model_name, search_space, mod in model_list:\n",
    "        def objective(search_space):\n",
    "            warnings.filterwarnings('ignore')\n",
    "            model = mod.set_params(**search_space)\n",
    "            model.fit(X_train_text, y_train)\n",
    "            y_pred = model.predict(X_val_text)\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "        try:\n",
    "            best_params, score = optimize_model_v2(search_space, objective, 200)\n",
    "            mod.set_params(**best_params)\n",
    "            score *= -1\n",
    "            trained_models.append((model_name, mod))\n",
    "            #print(f\"Trained {model_name} on {category}\")\n",
    "        except:\n",
    "            print(f\"Error training {model_name}, skipping\")\n",
    "            continue\n",
    "    return trained_models\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_search_space={  \n",
    "                'C': hp.lognormal('C', 0, 1),\n",
    "                'kernel':hp.choice('kernel', [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "                'coef0':hp.uniform('coef0', 0.0, 1.0),\n",
    "                'shrinking':hp.choice('shrinking', [True, False]),\n",
    "                'tol':hp.loguniform('tol', np.log(1e-5), np.log(1e-2)),\n",
    "                'degree':hp.choice('degree', [1, 2, 3, 4, 5]),\n",
    "                'gamma':hp.choice('gamma', [\"scale\", \"auto\"]),\n",
    "                }\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_search_space={\n",
    "                \"n_neighbors\":hp.choice('n_neighbors', np.arange(1, 16, dtype=int)),\n",
    "                \"algorithm\":hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "                \"metric\": hp.choice(\"metric\", [\"cityblock\", \"l1\", \"l2\", \"minkowski\", \"euclidean\", \"manhattan\"]),\n",
    "                \"p\":hp.uniform(\"p\", 1, 5)\n",
    "                }\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "LR_search_space={\n",
    "                'C': hp.lognormal('C', 0, 1),\n",
    "                'penalty':hp.choice('p_saga',['elasticnet','l1','l2',None]),\n",
    "                'tol': hp.loguniform('tol',-13,-1),\n",
    "                'l1_ratio': hp.uniform('l1_ratio',0,1)\n",
    "                }\n",
    "\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_search_space={  'n_estimators':hp.randint('n_estimators',200,1000),\n",
    "                'max_depth': hp.randint('max_depth',10,200),                      \n",
    "                'min_samples_split':hp.uniform('min_samples_split',0,1),   \n",
    "                'min_samples_leaf':hp.randint('min_samples_leaf',1,10),            \n",
    "                'criterion':hp.choice('criterion',['gini','entropy']),               \n",
    "                'max_features':hp.choice('max_features',['sqrt', 'log2']) }\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_search_space={\n",
    "                'activation':hp.choice('activation', [\"identity\",\"logistic\",\"tanh\",\"relu\"]),\n",
    "                'solver':hp.choice('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "                'alpha':hp.uniform(\"alpha\", 1e-4, 0.01),\n",
    "                'learning_rate':hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "                'learning_rate_init':hp.uniform(\"learning_rate_init\", 1e-4, 0.1),\n",
    "                'power_t':hp.uniform('power_t', 0.1, 0.9),\n",
    "                'tol':hp.uniform('tol', 1e-4, 0.01),\n",
    "                'momentum':hp.uniform('momentum', 0.8, 1.0),\n",
    "                'early_stopping':hp.choice('early_stopping', [True, False]),\n",
    "                'beta_1':hp.uniform(\"beta_1\", 0.8, 1.0),\n",
    "                'beta_2':hp.uniform(\"beta_2\", 0.95, 1.0),\n",
    "                'epsilon':hp.uniform(\"epsilon\", 1e-9, 1e-5)\n",
    "                }\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_search_space={\n",
    "                'var_smoothing': 10**-9\n",
    "                }\n",
    "\n",
    "# SGD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGD_search_space={\n",
    "                'loss':hp.choice('loss',[\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]),\n",
    "                'penalty':hp.choice(\"penalty\", [\"l2\", \"l1\", \"elasticnet\", None]),\n",
    "                'alpha':hp.loguniform(\"alpha\", np.log(1e-6), np.log(1e-1)),\n",
    "                \"l1_ratio\":hp.loguniform(\"l1_ratio\", np.log(1e-7), np.log(1)),\n",
    "                \"tol\":hp.loguniform(\"tol\", np.log(1e-5), np.log(1e-2)),\n",
    "                'learning_rate':hp.choice(\"learning_rate\",  [\"optimal\", \"invscaling\", \"constant\", \"adaptive\"]),\n",
    "                'eta0':hp.loguniform(\"eta0\", np.log(1e-5), np.log(1e-1))\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_v2 = [\n",
    "    (\"SVM\", SVM_search_space, SVC(random_state=42)),\n",
    "    (\"KNN\", KNN_search_space, KNeighborsClassifier(n_jobs=-1)),\n",
    "    (\"Logistic Regression\", LR_search_space, LogisticRegression(solver=\"saga\", max_iter=1000, random_state=42, n_jobs=-1)),\n",
    "    (\"Random Forest\", RF_search_space, RandomForestClassifier()),\n",
    "    (\"MLP\", MLP_search_space, MLPClassifier()),\n",
    "    (\"Gaussian NB\", NB_search_space, GaussianNB()),\n",
    "    (\"SGD\", SGD_search_space, SGDClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme = get_dataset(\"pheme\")\n",
    "X = pheme.drop('target', axis=1)\n",
    "y = pheme['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "pheme_train = pd.concat([X_train, y_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(dataset):\n",
    "    d = get_dataset(dataset)\n",
    "    X = d.drop('target', axis=1)\n",
    "    y = d['target']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "    train_d = pd.concat([X_train, y_train], axis=1)\n",
    "    val_d = pd.concat([X_val, y_val], axis=1)\n",
    "    return train_d, val_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = baseline(pheme, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = get_dataset(\"twitter\")\n",
    "#b = baseline(twitter, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo = get_dataset(\"weibo\")\n",
    "#c = baseline(weibo, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_t, twitter_v = arrange_data(\"twitter\")\n",
    "weibo_t, weibo_v = arrange_data(\"weibo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 27/200 [00:23<02:51,  1.01trial/s, best loss: -0.7873651771956857]"
     ]
    }
   ],
   "source": [
    "models, all_models = train_models(\"pheme\", pheme_train, 0.3, 50, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = get_dataset(\"twitter\")\n",
    "twitter_models, all_twitter_models = train_models(\"twitter\", twitter_t, 0.3, 50, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twitter_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m twitter_models\n",
      "\u001b[0;31mNameError\u001b[0m: name 'twitter_models' is not defined"
     ]
    }
   ],
   "source": [
    "twitter_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo = get_dataset(\"weibo\")\n",
    "weibo_models, all_weibo_models = train_models(\"weibo\", weibo_t, 0.3, 50, model_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [models, twitter_models, weibo_models]\n",
    "for k in a:\n",
    "    with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        for key, value in k.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 85/200 [02:00<02:42,  1.42s/trial, best loss: -0.8196125907990315]\n",
      "100%|██████████| 200/200 [01:25<00:00,  2.33trial/s, best loss: -0.7941888619854721]\n",
      " 53%|█████▎    | 106/200 [02:00<01:46,  1.14s/trial, best loss: -0.7772397094430993]\n",
      " 12%|█▏        | 24/200 [02:02<15:01,  5.12s/trial, best loss: -0.7699757869249395]\n",
      " 68%|██████▊   | 137/200 [02:00<00:55,  1.14trial/s, best loss: -0.8280871670702179]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.02trial/s, best loss: -0.7336561743341404]\n",
      "100%|██████████| 200/200 [01:43<00:00,  1.94trial/s, best loss: -0.7784503631961259]\n",
      "Trained models on Sensitive Subjects, added SVM to list\n",
      " 44%|████▍     | 88/200 [04:05<05:12,  2.79s/trial, best loss: -0.8403614457831325]  \n",
      "100%|██████████| 200/200 [01:12<00:00,  2.75trial/s, best loss: -0.802710843373494]\n",
      "100%|██████████| 200/200 [01:51<00:00,  1.79trial/s, best loss: -0.786144578313253]\n",
      " 11%|█         | 22/200 [02:04<16:49,  5.67s/trial, best loss: -0.7876506024096386]\n",
      "100%|██████████| 200/200 [01:48<00:00,  1.84trial/s, best loss: -0.822289156626506]\n",
      "100%|██████████| 200/200 [00:01<00:00, 108.72trial/s, best loss: -0.7319277108433735]\n",
      "100%|██████████| 200/200 [01:03<00:00,  3.17trial/s, best loss: -0.7876506024096386]\n",
      "Trained models on News, added KNN to list\n",
      "100%|██████████| 200/200 [00:38<00:00,  5.23trial/s, best loss: -0.8735177865612648]\n",
      "100%|██████████| 200/200 [00:31<00:00,  6.28trial/s, best loss: -0.8458498023715415]\n",
      " 54%|█████▎    | 107/200 [02:01<01:45,  1.14s/trial, best loss: -0.8102766798418972]\n",
      " 26%|██▋       | 53/200 [02:01<05:38,  2.30s/trial, best loss: -0.8181818181818182]\n",
      "100%|██████████| 200/200 [00:59<00:00,  3.36trial/s, best loss: -0.8656126482213439]\n",
      "100%|██████████| 200/200 [00:00<00:00, 204.95trial/s, best loss: -0.7430830039525692]\n",
      "100%|██████████| 200/200 [00:27<00:00,  7.23trial/s, best loss: -0.8142292490118577]\n",
      "Trained models on Arts & Entertainment, added KNN to list\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.94trial/s, best loss: -0.8556338028169014]\n",
      "100%|██████████| 200/200 [00:33<00:00,  6.04trial/s, best loss: -0.8556338028169014]\n",
      "100%|██████████| 200/200 [01:53<00:00,  1.77trial/s, best loss: -0.8415492957746479]\n",
      " 26%|██▋       | 53/200 [02:01<05:37,  2.30s/trial, best loss: -0.8133802816901409]\n",
      "100%|██████████| 200/200 [00:48<00:00,  4.12trial/s, best loss: -0.8732394366197183]\n",
      "100%|██████████| 200/200 [00:00<00:00, 202.99trial/s, best loss: -0.7676056338028169]\n",
      "100%|██████████| 200/200 [00:26<00:00,  7.68trial/s, best loss: -0.8556338028169014]\n",
      "Trained models on People & Society, added KNN to list\n",
      "100%|██████████| 200/200 [00:36<00:00,  5.45trial/s, best loss: -0.7740863787375415]\n",
      "100%|██████████| 200/200 [00:55<00:00,  3.58trial/s, best loss: -0.7508305647840532]\n",
      "100%|██████████| 200/200 [01:15<00:00,  2.67trial/s, best loss: -0.7574750830564784]\n",
      " 21%|██        | 42/200 [02:01<07:35,  2.88s/trial, best loss: -0.7740863787375415]\n",
      "100%|██████████| 200/200 [01:21<00:00,  2.46trial/s, best loss: -0.7674418604651163]\n",
      "100%|██████████| 200/200 [00:01<00:00, 192.88trial/s, best loss: -0.7043189368770764]\n",
      "100%|██████████| 200/200 [00:52<00:00,  3.79trial/s, best loss: -0.7541528239202658]\n",
      "Trained models on Law & Government, added KNN to list\n",
      "100%|██████████| 200/200 [00:26<00:00,  7.60trial/s, best loss: -0.8717948717948718]\n",
      "100%|██████████| 200/200 [00:28<00:00,  7.00trial/s, best loss: -0.8653846153846154]\n",
      "100%|██████████| 200/200 [01:12<00:00,  2.74trial/s, best loss: -0.8589743589743589]\n",
      " 22%|██▏       | 43/200 [02:04<07:33,  2.89s/trial, best loss: -0.8173076923076923]\n",
      "100%|██████████| 200/200 [01:18<00:00,  2.56trial/s, best loss: -0.875]             \n",
      "100%|██████████| 200/200 [00:01<00:00, 183.68trial/s, best loss: -0.75]\n",
      "100%|██████████| 200/200 [00:35<00:00,  5.68trial/s, best loss: -0.8557692307692307]\n",
      "Trained models on Online Communities, added KNN to list\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 75.53trial/s, best loss: -0.9142857142857143]\n",
      "100%|██████████| 200/200 [00:05<00:00, 34.87trial/s, best loss: -0.9428571428571428]\n",
      "100%|██████████| 200/200 [00:37<00:00,  5.33trial/s, best loss: -0.9142857142857143]\n",
      " 64%|██████▍   | 128/200 [02:01<01:08,  1.06trial/s, best loss: -0.8571428571428571]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.98trial/s, best loss: -0.9142857142857143]\n",
      "100%|██████████| 200/200 [00:00<00:00, 341.49trial/s, best loss: -0.6571428571428571]\n",
      "100%|██████████| 200/200 [00:07<00:00, 25.14trial/s, best loss: -0.9142857142857143]\n",
      "Trained models on Reference, added Logistic Regression to list\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 83.88trial/s, best loss: -0.8421052631578947]\n",
      "100%|██████████| 200/200 [00:03<00:00, 55.70trial/s, best loss: -0.8947368421052632]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.43trial/s, best loss: -0.8421052631578947]\n",
      " 82%|████████▏ | 163/200 [02:00<00:27,  1.35trial/s, best loss: -0.7368421052631579]\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.12trial/s, best loss: -0.8421052631578947]\n",
      "100%|██████████| 200/200 [00:00<00:00, 358.70trial/s, best loss: -0.47368421052631576]\n",
      "100%|██████████| 200/200 [00:05<00:00, 35.93trial/s, best loss: -0.8421052631578947]\n",
      "Trained models on Health, added KNN to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 82.69trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:03<00:00, 60.11trial/s, best loss: -0.9444444444444444]\n",
      "100%|██████████| 200/200 [00:22<00:00,  9.05trial/s, best loss: -0.8888888888888888]\n",
      " 79%|███████▉  | 158/200 [02:00<00:31,  1.31trial/s, best loss: -0.8888888888888888]\n",
      "100%|██████████| 200/200 [00:17<00:00, 11.20trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:00<00:00, 356.77trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:03<00:00, 52.61trial/s, best loss: -0.8888888888888888]\n",
      "Trained models on Business & Industrial, added SVM to list\n",
      "Skipped category: Autos & Vehicles due to low numbers\n",
      "Skipped category: Food & Drink due to low numbers\n",
      "100%|██████████| 200/200 [00:04<00:00, 49.09trial/s, best loss: -0.8873239436619719]\n",
      "100%|██████████| 200/200 [00:06<00:00, 29.48trial/s, best loss: -0.8028169014084507]\n",
      "100%|██████████| 200/200 [01:44<00:00,  1.91trial/s, best loss: -0.9014084507042254]\n",
      " 52%|█████▏    | 104/200 [02:00<01:51,  1.16s/trial, best loss: -0.8309859154929577]\n",
      "100%|██████████| 200/200 [00:33<00:00,  6.03trial/s, best loss: -0.9154929577464789]\n",
      "100%|██████████| 200/200 [00:00<00:00, 300.61trial/s, best loss: -0.7323943661971831]\n",
      "100%|██████████| 200/200 [00:07<00:00, 26.09trial/s, best loss: -0.8873239436619719]\n",
      "Trained models on Travel & Transportation, added SGD to list\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Science due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "100%|██████████| 200/200 [00:05<00:00, 37.78trial/s, best loss: -0.8255813953488372]\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.04trial/s, best loss: -0.7790697674418605]\n",
      "100%|██████████| 200/200 [00:25<00:00,  7.92trial/s, best loss: -0.7790697674418605]\n",
      " 40%|████      | 80/200 [02:00<03:00,  1.50s/trial, best loss: -0.7558139534883721]\n",
      "100%|██████████| 200/200 [00:25<00:00,  7.85trial/s, best loss: -0.813953488372093]\n",
      "100%|██████████| 200/200 [00:00<00:00, 289.46trial/s, best loss: -0.7674418604651163]\n",
      "100%|██████████| 200/200 [00:08<00:00, 23.74trial/s, best loss: -0.8255813953488372]\n",
      "Trained models on People & Society, added SVM to list\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.14trial/s, best loss: -0.8359375]\n",
      "100%|██████████| 200/200 [00:10<00:00, 18.40trial/s, best loss: -0.765625]\n",
      "100%|██████████| 200/200 [00:53<00:00,  3.74trial/s, best loss: -0.7890625]\n",
      " 33%|███▎      | 66/200 [02:00<04:05,  1.83s/trial, best loss: -0.8125]   \n",
      "100%|██████████| 200/200 [00:37<00:00,  5.30trial/s, best loss: -0.8359375]\n",
      "100%|██████████| 200/200 [00:00<00:00, 279.20trial/s, best loss: -0.6953125]\n",
      "100%|██████████| 200/200 [00:24<00:00,  8.19trial/s, best loss: -0.78125] \n",
      "Trained models on Arts & Entertainment, added Random Forest to list\n",
      "100%|██████████| 200/200 [00:08<00:00, 24.58trial/s, best loss: -0.8090909090909091]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.70trial/s, best loss: -0.8545454545454545]\n",
      " 85%|████████▌ | 170/200 [02:00<00:21,  1.41trial/s, best loss: -0.8]               \n",
      " 41%|████      | 82/200 [02:02<02:55,  1.49s/trial, best loss: -0.7454545454545455]\n",
      "100%|██████████| 200/200 [00:48<00:00,  4.15trial/s, best loss: -0.8090909090909091]\n",
      "100%|██████████| 200/200 [00:00<00:00, 269.24trial/s, best loss: -0.7090909090909091]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.16trial/s, best loss: -0.8090909090909091]\n",
      "Trained models on Law & Government, added KNN to list\n",
      "100%|██████████| 200/200 [00:22<00:00,  9.03trial/s, best loss: -0.827906976744186]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.76trial/s, best loss: -0.8232558139534883]\n",
      "100%|██████████| 200/200 [00:59<00:00,  3.38trial/s, best loss: -0.7674418604651163]\n",
      " 25%|██▌       | 50/200 [02:00<06:00,  2.41s/trial, best loss: -0.7627906976744186]\n",
      "100%|██████████| 200/200 [00:55<00:00,  3.59trial/s, best loss: -0.8465116279069768]\n",
      "100%|██████████| 200/200 [00:00<00:00, 222.10trial/s, best loss: -0.7023255813953488]\n",
      "100%|██████████| 200/200 [00:38<00:00,  5.19trial/s, best loss: -0.7813953488372093]\n",
      "Trained models on News, added MLP to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.20trial/s, best loss: -0.9444444444444444]\n",
      "100%|██████████| 200/200 [00:03<00:00, 50.88trial/s, best loss: -1.0]              \n",
      "100%|██████████| 200/200 [00:28<00:00,  6.90trial/s, best loss: -1.0]              \n",
      " 82%|████████▎ | 165/200 [02:00<00:25,  1.37trial/s, best loss: -0.8888888888888888]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.05trial/s, best loss: -1.0]              \n",
      "100%|██████████| 200/200 [00:00<00:00, 362.03trial/s, best loss: -0.8888888888888888]\n",
      "100%|██████████| 200/200 [00:03<00:00, 56.37trial/s, best loss: -1.0]              \n",
      "Trained models on Food & Drink, added KNN to list\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.84trial/s, best loss: -0.8146067415730337]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.96trial/s, best loss: -0.8539325842696629]\n",
      "100%|██████████| 200/200 [01:17<00:00,  2.57trial/s, best loss: -0.7528089887640449]\n",
      " 30%|███       | 61/200 [02:00<04:33,  1.97s/trial, best loss: -0.7865168539325843]\n",
      "100%|██████████| 200/200 [00:53<00:00,  3.74trial/s, best loss: -0.8146067415730337]\n",
      "100%|██████████| 200/200 [00:00<00:00, 246.21trial/s, best loss: -0.6404494382022472]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.90trial/s, best loss: -0.7415730337078652]\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "100%|██████████| 200/200 [00:04<00:00, 46.52trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.64trial/s, best loss: -0.7692307692307693]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.85trial/s, best loss: -0.7692307692307693]\n",
      " 47%|████▋     | 94/200 [02:01<02:17,  1.29s/trial, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:29<00:00,  6.89trial/s, best loss: -0.8205128205128205]\n",
      "100%|██████████| 200/200 [00:00<00:00, 300.25trial/s, best loss: -0.7307692307692307]\n",
      "100%|██████████| 200/200 [00:11<00:00, 17.64trial/s, best loss: -0.8076923076923077]\n",
      "Trained models on Online Communities, added Random Forest to list\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 80.94trial/s, best loss: -0.7391304347826086]\n",
      "100%|██████████| 200/200 [00:03<00:00, 54.97trial/s, best loss: -0.8695652173913043]\n",
      "100%|██████████| 200/200 [00:26<00:00,  7.51trial/s, best loss: -0.6956521739130435]\n",
      " 78%|███████▊  | 156/200 [02:00<00:33,  1.30trial/s, best loss: -0.6956521739130435]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.81trial/s, best loss: -0.7391304347826086]\n",
      "100%|██████████| 200/200 [00:00<00:00, 339.88trial/s, best loss: -0.6521739130434783]\n",
      "100%|██████████| 200/200 [00:04<00:00, 49.62trial/s, best loss: -0.7391304347826086]\n",
      "Trained models on Health, added KNN to list\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Reference due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 83.87trial/s, best loss: -0.8]\n",
      "100%|██████████| 200/200 [00:04<00:00, 46.06trial/s, best loss: -0.8666666666666667]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.93trial/s, best loss: -0.8]\n",
      " 78%|███████▊  | 155/200 [02:00<00:34,  1.29trial/s, best loss: -0.8666666666666667]\n",
      "100%|██████████| 200/200 [00:11<00:00, 17.13trial/s, best loss: -0.9333333333333333]\n",
      "100%|██████████| 200/200 [00:00<00:00, 348.65trial/s, best loss: -0.7333333333333333]\n",
      "100%|██████████| 200/200 [00:03<00:00, 53.79trial/s, best loss: -0.8666666666666667]\n",
      "Trained models on Business & Industrial, added SVM to list\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n",
      "Skipped category: Science due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Travel & Transportation due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 83.75trial/s, best loss: -0.9444444444444444]\n",
      "100%|██████████| 200/200 [00:04<00:00, 42.51trial/s, best loss: -1.0]              \n",
      "100%|██████████| 200/200 [00:17<00:00, 11.46trial/s, best loss: -0.9444444444444444]\n",
      " 75%|███████▌  | 150/200 [02:00<00:40,  1.24trial/s, best loss: -0.8888888888888888]\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.07trial/s, best loss: -0.9444444444444444]\n",
      "100%|██████████| 200/200 [00:00<00:00, 358.58trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:04<00:00, 46.95trial/s, best loss: -1.0]              \n",
      "Trained models on Sports, added KNN to list\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "100%|██████████| 200/200 [00:31<00:00,  6.42trial/s, best loss: -0.8190045248868778]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.74trial/s, best loss: -0.751131221719457]\n",
      "100%|██████████| 200/200 [00:34<00:00,  5.80trial/s, best loss: -0.7873303167420814]\n",
      " 30%|███       | 60/200 [02:00<04:41,  2.01s/trial, best loss: -0.8371040723981901]\n",
      "100%|██████████| 200/200 [01:11<00:00,  2.81trial/s, best loss: -0.8235294117647058]\n",
      "100%|██████████| 200/200 [00:00<00:00, 224.90trial/s, best loss: -0.6470588235294118]\n",
      "100%|██████████| 200/200 [01:00<00:00,  3.30trial/s, best loss: -0.7918552036199095]\n",
      "Trained models on Arts & Entertainment, added Random Forest to list\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.96trial/s, best loss: -0.8473282442748091]\n",
      "100%|██████████| 200/200 [00:10<00:00, 18.50trial/s, best loss: -0.816793893129771]\n",
      "100%|██████████| 200/200 [01:15<00:00,  2.64trial/s, best loss: -0.8320610687022901]\n",
      " 40%|████      | 80/200 [02:01<03:01,  1.51s/trial, best loss: -0.8396946564885496]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.90trial/s, best loss: -0.8931297709923665]\n",
      "100%|██████████| 200/200 [00:00<00:00, 272.62trial/s, best loss: -0.6946564885496184]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.78trial/s, best loss: -0.8396946564885496]\n",
      "Trained models on People & Society, added SVM to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 68.80trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:06<00:00, 30.69trial/s, best loss: -0.8809523809523809]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.92trial/s, best loss: -0.8571428571428571]\n",
      " 64%|██████▎   | 127/200 [02:00<01:09,  1.06trial/s, best loss: -0.9285714285714286]\n",
      "100%|██████████| 200/200 [00:24<00:00,  8.32trial/s, best loss: -0.9047619047619048]\n",
      "100%|██████████| 200/200 [00:00<00:00, 335.36trial/s, best loss: -0.7142857142857143]\n",
      "100%|██████████| 200/200 [00:06<00:00, 31.88trial/s, best loss: -0.9047619047619048]\n",
      "Trained models on Reference, added MLP to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 69.95trial/s, best loss: -0.8913043478260869]\n",
      "100%|██████████| 200/200 [00:04<00:00, 42.52trial/s, best loss: -0.8043478260869565]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.81trial/s, best loss: -0.8695652173913043]\n",
      " 63%|██████▎   | 126/200 [02:00<01:10,  1.05trial/s, best loss: -0.8043478260869565]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.78trial/s, best loss: -0.8478260869565217]\n",
      "100%|██████████| 200/200 [00:00<00:00, 336.66trial/s, best loss: -0.8043478260869565]\n",
      "100%|██████████| 200/200 [00:06<00:00, 28.73trial/s, best loss: -0.8913043478260869]\n",
      "Trained models on Food & Drink, added SGD to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 85.65trial/s, best loss: -1.0]\n",
      "100%|██████████| 200/200 [00:03<00:00, 50.07trial/s, best loss: -0.7894736842105263]\n",
      "100%|██████████| 200/200 [00:24<00:00,  8.07trial/s, best loss: -0.9473684210526315]\n",
      " 79%|███████▉  | 158/200 [02:00<00:31,  1.31trial/s, best loss: -0.8421052631578947]\n",
      "100%|██████████| 200/200 [00:17<00:00, 11.12trial/s, best loss: -1.0]\n",
      "100%|██████████| 200/200 [00:00<00:00, 354.50trial/s, best loss: -0.7368421052631579]\n",
      "100%|██████████| 200/200 [00:03<00:00, 51.67trial/s, best loss: -1.0]              \n",
      "Trained models on Sports, added SVM to list\n",
      "Skipped category: Games due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 75.66trial/s, best loss: -1.0]               \n",
      "100%|██████████| 200/200 [00:05<00:00, 36.81trial/s, best loss: -0.9032258064516129]\n",
      "100%|██████████| 200/200 [00:12<00:00, 15.78trial/s, best loss: -0.967741935483871]\n",
      " 73%|███████▎  | 146/200 [02:00<00:44,  1.21trial/s, best loss: -0.9354838709677419]\n",
      "100%|██████████| 200/200 [00:21<00:00,  9.22trial/s, best loss: -0.967741935483871]\n",
      "100%|██████████| 200/200 [00:00<00:00, 346.66trial/s, best loss: -0.7419354838709677]\n",
      "100%|██████████| 200/200 [00:06<00:00, 32.81trial/s, best loss: -0.967741935483871]\n",
      "Trained models on Travel & Transportation, added KNN to list\n",
      "100%|██████████| 200/200 [00:03<00:00, 59.09trial/s, best loss: -0.9137931034482759]\n",
      "100%|██████████| 200/200 [00:06<00:00, 32.66trial/s, best loss: -0.8103448275862069]\n",
      "100%|██████████| 200/200 [01:10<00:00,  2.83trial/s, best loss: -0.8793103448275862]\n",
      " 58%|█████▊    | 116/200 [02:01<01:27,  1.04s/trial, best loss: -0.8448275862068966]\n",
      "100%|██████████| 200/200 [00:34<00:00,  5.78trial/s, best loss: -0.9137931034482759]\n",
      "100%|██████████| 200/200 [00:00<00:00, 324.19trial/s, best loss: -0.7931034482758621]\n",
      "100%|██████████| 200/200 [00:06<00:00, 30.54trial/s, best loss: -0.9310344827586207]\n",
      "Trained models on Health, added Logistic Regression to list\n",
      "100%|██████████| 200/200 [00:03<00:00, 54.84trial/s, best loss: -0.9090909090909091]\n",
      "100%|██████████| 200/200 [00:08<00:00, 23.14trial/s, best loss: -0.8939393939393939]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.90trial/s, best loss: -0.9090909090909091]\n",
      " 48%|████▊     | 96/200 [02:00<02:10,  1.26s/trial, best loss: -0.8939393939393939]\n",
      "100%|██████████| 200/200 [00:40<00:00,  5.00trial/s, best loss: -0.9090909090909091]\n",
      "100%|██████████| 200/200 [00:00<00:00, 318.80trial/s, best loss: -0.696969696969697]\n",
      "100%|██████████| 200/200 [00:08<00:00, 23.77trial/s, best loss: -0.9090909090909091]\n",
      "Trained models on Online Communities, added KNN to list\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.62trial/s, best loss: -0.825136612021858]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.97trial/s, best loss: -0.7868852459016393]\n",
      "100%|██████████| 200/200 [00:58<00:00,  3.39trial/s, best loss: -0.8360655737704918]\n",
      " 24%|██▍       | 48/200 [02:01<06:23,  2.52s/trial, best loss: -0.7704918032786885]\n",
      "100%|██████████| 200/200 [01:33<00:00,  2.14trial/s, best loss: -0.8306010928961749]\n",
      "100%|██████████| 200/200 [00:00<00:00, 237.18trial/s, best loss: -0.7595628415300546]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.75trial/s, best loss: -0.8415300546448088]\n",
      "Trained models on News, added SVM to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 88.90trial/s, best loss: -1.0]\n",
      "100%|██████████| 200/200 [00:03<00:00, 52.89trial/s, best loss: -1.0]              \n",
      "100%|██████████| 200/200 [00:19<00:00, 10.44trial/s, best loss: -0.9473684210526315]\n",
      " 76%|███████▋  | 153/200 [02:00<00:36,  1.27trial/s, best loss: -1.0]\n",
      "100%|██████████| 200/200 [00:14<00:00, 14.07trial/s, best loss: -1.0]               \n",
      "100%|██████████| 200/200 [00:00<00:00, 335.53trial/s, best loss: -0.9473684210526315]\n",
      "100%|██████████| 200/200 [00:04<00:00, 46.27trial/s, best loss: -0.9473684210526315]\n",
      "Trained models on Science, added SVM to list\n",
      "100%|██████████| 200/200 [00:12<00:00, 16.43trial/s, best loss: -0.8448275862068966]\n",
      "100%|██████████| 200/200 [00:29<00:00,  6.82trial/s, best loss: -0.8505747126436781]\n",
      "100%|██████████| 200/200 [00:48<00:00,  4.09trial/s, best loss: -0.8275862068965517]\n",
      " 29%|██▉       | 58/200 [02:01<04:56,  2.09s/trial, best loss: -0.7988505747126436]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.91trial/s, best loss: -0.867816091954023] \n",
      "100%|██████████| 200/200 [00:00<00:00, 254.46trial/s, best loss: -0.7701149425287356]\n",
      "100%|██████████| 200/200 [00:16<00:00, 11.98trial/s, best loss: -0.8160919540229885]\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 80.72trial/s, best loss: -0.9130434782608695]\n",
      "100%|██████████| 200/200 [00:04<00:00, 47.17trial/s, best loss: -0.9130434782608695]\n",
      "100%|██████████| 200/200 [00:29<00:00,  6.89trial/s, best loss: -0.8695652173913043]\n",
      " 76%|███████▌  | 152/200 [02:00<00:38,  1.26trial/s, best loss: -0.8695652173913043]\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.23trial/s, best loss: -0.8695652173913043]\n",
      "100%|██████████| 200/200 [00:00<00:00, 353.82trial/s, best loss: -0.6086956521739131]\n",
      "100%|██████████| 200/200 [00:05<00:00, 37.07trial/s, best loss: -0.8695652173913043]\n",
      "Trained models on Shopping, added SVM to list\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 78.78trial/s, best loss: -0.9354838709677419]\n",
      "100%|██████████| 200/200 [00:05<00:00, 37.66trial/s, best loss: -0.9032258064516129]\n",
      "100%|██████████| 200/200 [00:43<00:00,  4.62trial/s, best loss: -0.8709677419354839]\n",
      " 68%|██████▊   | 135/200 [02:00<00:58,  1.12trial/s, best loss: -0.8709677419354839]\n",
      "100%|██████████| 200/200 [00:23<00:00,  8.56trial/s, best loss: -0.9032258064516129]\n",
      "100%|██████████| 200/200 [00:00<00:00, 327.04trial/s, best loss: -0.7741935483870968]\n",
      "100%|██████████| 200/200 [00:06<00:00, 28.93trial/s, best loss: -0.9354838709677419]\n",
      "Trained models on Jobs & Education, added MLP to list\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.24trial/s, best loss: -0.8716216216216216]\n",
      "100%|██████████| 200/200 [00:11<00:00, 18.09trial/s, best loss: -0.8243243243243243]\n",
      "100%|██████████| 200/200 [01:53<00:00,  1.76trial/s, best loss: -0.8378378378378378]\n",
      " 29%|██▉       | 58/200 [02:00<04:54,  2.08s/trial, best loss: -0.7972972972972973]\n",
      "100%|██████████| 200/200 [00:41<00:00,  4.86trial/s, best loss: -0.8716216216216216]\n",
      "100%|██████████| 200/200 [00:00<00:00, 263.14trial/s, best loss: -0.7702702702702703]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.90trial/s, best loss: -0.8648648648648649]\n",
      "Trained models on Law & Government, added SVM to list\n",
      "100%|██████████| 200/200 [00:03<00:00, 57.43trial/s, best loss: -0.7647058823529411]\n",
      "100%|██████████| 200/200 [00:04<00:00, 41.54trial/s, best loss: -0.7647058823529411]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.28trial/s, best loss: -0.7450980392156863]\n",
      " 64%|██████▎   | 127/200 [02:00<01:09,  1.05trial/s, best loss: -0.803921568627451]\n",
      "100%|██████████| 200/200 [00:20<00:00,  9.56trial/s, best loss: -0.8235294117647058]\n",
      "100%|██████████| 200/200 [00:00<00:00, 328.38trial/s, best loss: -0.7254901960784313]\n",
      "100%|██████████| 200/200 [00:08<00:00, 22.32trial/s, best loss: -0.7450980392156863]\n",
      "Trained models on Business & Industrial, added KNN to list\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 91.21trial/s, best loss: -0.8888888888888888]\n",
      "100%|██████████| 200/200 [00:03<00:00, 54.95trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:23<00:00,  8.69trial/s, best loss: -0.8888888888888888]\n",
      " 78%|███████▊  | 155/200 [02:00<00:35,  1.28trial/s, best loss: -0.7777777777777778]\n",
      "100%|██████████| 200/200 [00:17<00:00, 11.37trial/s, best loss: -0.8888888888888888]\n",
      "100%|██████████| 200/200 [00:00<00:00, 349.71trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:04<00:00, 48.85trial/s, best loss: -0.8888888888888888]\n",
      "Trained models on Internet & Telecom, added SGD to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 79.45trial/s, best loss: -0.9642857142857143]\n",
      "100%|██████████| 200/200 [00:04<00:00, 46.50trial/s, best loss: -0.7857142857142857]\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.30trial/s, best loss: -0.8928571428571429]\n",
      " 76%|███████▌  | 152/200 [02:00<00:37,  1.26trial/s, best loss: -0.8214285714285714]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.58trial/s, best loss: -0.9285714285714286]\n",
      "100%|██████████| 200/200 [00:00<00:00, 346.80trial/s, best loss: -0.75]\n",
      "100%|██████████| 200/200 [00:07<00:00, 26.94trial/s, best loss: -0.9285714285714286]\n",
      "Trained models on Hobbies & Leisure, added Logistic Regression to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 74.63trial/s, best loss: -0.8571428571428571]\n",
      "100%|██████████| 200/200 [00:04<00:00, 42.56trial/s, best loss: -0.8571428571428571]\n",
      "100%|██████████| 200/200 [00:24<00:00,  8.30trial/s, best loss: -0.8571428571428571]\n",
      " 74%|███████▍  | 148/200 [02:00<00:42,  1.22trial/s, best loss: -0.8285714285714286]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.85trial/s, best loss: -0.8857142857142857]\n",
      "100%|██████████| 200/200 [00:00<00:00, 320.72trial/s, best loss: -0.6857142857142857]\n",
      "100%|██████████| 200/200 [00:07<00:00, 26.04trial/s, best loss: -0.8571428571428571]\n",
      "Trained models on Books & Literature, added KNN to list\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 81.88trial/s, best loss: -0.8260869565217391]\n",
      "100%|██████████| 200/200 [00:04<00:00, 47.70trial/s, best loss: -0.7391304347826086]\n",
      "100%|██████████| 200/200 [00:18<00:00, 11.02trial/s, best loss: -0.782608695652174]\n",
      " 78%|███████▊  | 156/200 [02:01<00:34,  1.29trial/s, best loss: -0.7391304347826086]\n",
      "100%|██████████| 200/200 [00:13<00:00, 15.29trial/s, best loss: -0.9130434782608695]\n",
      "100%|██████████| 200/200 [00:00<00:00, 331.45trial/s, best loss: -0.7391304347826086]\n",
      "100%|██████████| 200/200 [00:04<00:00, 44.93trial/s, best loss: -0.9130434782608695]\n",
      "Trained models on Pets & Animals, added KNN to list\n",
      "100%|██████████| 200/200 [00:02<00:00, 84.82trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:03<00:00, 62.33trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.79trial/s, best loss: -0.7777777777777778]\n",
      " 80%|████████  | 161/200 [02:00<00:29,  1.33trial/s, best loss: -0.8333333333333334]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.51trial/s, best loss: -0.8888888888888888]\n",
      "100%|██████████| 200/200 [00:00<00:00, 332.05trial/s, best loss: -0.7777777777777778]\n",
      "100%|██████████| 200/200 [00:03<00:00, 52.13trial/s, best loss: -0.8333333333333334]\n",
      "Trained models on Adult, added MLP to list\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "100%|██████████| 200/200 [00:02<00:00, 80.76trial/s, best loss: -0.84375]\n",
      "100%|██████████| 200/200 [00:05<00:00, 33.42trial/s, best loss: -0.96875]\n",
      "100%|██████████| 200/200 [00:29<00:00,  6.81trial/s, best loss: -0.8125]\n",
      " 70%|███████   | 140/200 [02:00<00:51,  1.16trial/s, best loss: -0.8125]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.64trial/s, best loss: -0.875] \n",
      "100%|██████████| 200/200 [00:00<00:00, 325.25trial/s, best loss: -0.84375]\n",
      "100%|██████████| 200/200 [00:07<00:00, 28.24trial/s, best loss: -0.875] \n",
      "Trained models on Autos & Vehicles, added KNN to list\n"
     ]
    }
   ],
   "source": [
    "pheme_models3, all_pheme_models3 = train_models(\"pheme\", pheme_train, 0, 100, model_list_v2)\n",
    "with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        for key, value in pheme_models3.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")\n",
    "twitter_models3, all_twitter_models3 = train_models(\"twitter\", twitter_t, 0, 100, model_list_v2)\n",
    "with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        for key, value in twitter_models3.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")\n",
    "weibo_models3, all_weibo_models3 = train_models(\"weibo\", weibo_t, 0, 100, model_list_v2)\n",
    "with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        for key, value in weibo_models3.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained models on Arts & Entertainment, added SVM to list\n",
      "Trained models on People & Society, added Logistic Regression to list\n",
      "Trained models on Reference, added KNN to list\n",
      "Trained models on Food & Drink, added Logistic Regression to list\n",
      "Trained models on Sports, added SVM to list\n",
      "Trained models on Games, added KNN to list\n",
      "Trained models on Travel & Transportation, added Logistic Regression to list\n",
      "Trained models on Health, added SGD to list\n",
      "Trained models on Online Communities, added SVM to list\n",
      "Trained models on News, added SVM to list\n",
      "Trained models on Science, added KNN to list\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Trained models on Shopping, added Logistic Regression to list\n",
      "Trained models on Finance, added KNN to list\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Trained models on Jobs & Education, added KNN to list\n",
      "Trained models on Law & Government, added SVM to list\n",
      "Trained models on Business & Industrial, added Random Forest to list\n",
      "Trained models on Computers & Electronics, added SVM to list\n",
      "Trained models on Internet & Telecom, added Logistic Regression to list\n",
      "Trained models on Hobbies & Leisure, added KNN to list\n",
      "Trained models on Books & Literature, added SVM to list\n",
      "Trained models on Beauty & Fitness, added SVM to list\n",
      "Trained models on Pets & Animals, added Logistic Regression to list\n",
      "Trained models on Adult, added KNN to list\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Trained models on Autos & Vehicles, added KNN to list\n"
     ]
    }
   ],
   "source": [
    "confidence_list = [0.1]\n",
    "\n",
    "for confidence in confidence_list:\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(f\"Weibo Models with {confidence} confidence and 50 size\\n\\n\")\n",
    "\n",
    "    \"\"\"pheme_models, all_pheme_models = train_models(\"pheme\", pheme_train, confidence, 50, model_list_v2)\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(\"PHEME\")\n",
    "        for key, value in pheme_models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")\"\"\"\n",
    "\n",
    "    \"\"\"twitter_models, all_twitter_models = train_models(\"twitter\", twitter_t, confidence, 50, model_list_v2)\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(\"Twitter\")\n",
    "        for key, value in twitter_models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")\"\"\"\n",
    "    \n",
    "    weibo_models, all_weibo_models = train_models(\"weibo\", weibo_t, confidence, 50, model_list_v2)\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(\"Weibo\")\n",
    "        for key, value in weibo_models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_list = [20, 100, 150, 200]\n",
    "\n",
    "for size in size_list:\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(f\"Weibo Models with 0.3 confidence and {size} size\\n\\n\")\n",
    "\n",
    "    pheme_models, all_pheme_models = train_models(\"weibo\", weibo, 0.3, size, model_list_v2)\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(\"Weibo\")\n",
    "        for key, value in pheme_models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained models on Arts & Entertainment, added Random Forest to list\n",
      "Trained models on People & Society, added MLP to list\n",
      "Trained models on Reference, added KNN to list\n",
      "Trained models on Food & Drink, added KNN to list\n",
      "Trained models on Sports, added MLP to list\n",
      "Trained models on Games, added KNN to list\n",
      "Trained models on Travel & Transportation, added MLP to list\n",
      "Trained models on Health, added SGD to list\n",
      "Trained models on Online Communities, added Logistic Regression to list\n",
      "Trained models on News, added Logistic Regression to list\n",
      "Trained models on Science, added Logistic Regression to list\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Trained models on Shopping, added KNN to list\n",
      "Trained models on Finance, added SVM to list\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Trained models on Jobs & Education, added KNN to list\n",
      "Trained models on Law & Government, added MLP to list\n",
      "Trained models on Business & Industrial, added SVM to list\n",
      "Trained models on Computers & Electronics, added KNN to list\n",
      "Trained models on Internet & Telecom, added KNN to list\n",
      "Trained models on Hobbies & Leisure, added Gaussian NB to list\n",
      "Trained models on Books & Literature, added KNN to list\n",
      "Trained models on Beauty & Fitness, added SVM to list\n",
      "Trained models on Pets & Animals, added Random Forest to list\n",
      "Trained models on Adult, added Gaussian NB to list\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Trained models on Autos & Vehicles, added SVM to list\n",
      "Trained models on Arts & Entertainment, added SGD to list\n",
      "Trained models on People & Society, added SVM to list\n",
      "Trained models on Reference, added KNN to list\n",
      "Trained models on Food & Drink, added KNN to list\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Trained models on Travel & Transportation, added MLP to list\n",
      "Trained models on Health, added Logistic Regression to list\n",
      "Trained models on Online Communities, added SGD to list\n",
      "Trained models on News, added Logistic Regression to list\n",
      "Skipped category: Science due to low numbers\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Trained models on Jobs & Education, added Random Forest to list\n",
      "Trained models on Law & Government, added MLP to list\n",
      "Trained models on Business & Industrial, added SVM to list\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Trained models on Hobbies & Leisure, added SVM to list\n",
      "Trained models on Books & Literature, added KNN to list\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Trained models on Pets & Animals, added SVM to list\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Trained models on Autos & Vehicles, added KNN to list\n",
      "Trained models on Arts & Entertainment, added SVM to list\n",
      "Trained models on People & Society, added KNN to list\n",
      "Skipped category: Reference due to low numbers\n",
      "Trained models on Food & Drink, added KNN to list\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Trained models on Travel & Transportation, added KNN to list\n",
      "Trained models on Health, added SVM to list\n",
      "Skipped category: Online Communities due to low numbers\n",
      "Trained models on News, added Logistic Regression to list\n",
      "Skipped category: Science due to low numbers\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Trained models on Law & Government, added MLP to list\n",
      "Trained models on Business & Industrial, added Logistic Regression to list\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Trained models on Autos & Vehicles, added KNN to list\n",
      "Trained models on Arts & Entertainment, added MLP to list\n",
      "Trained models on People & Society, added KNN to list\n",
      "Skipped category: Reference due to low numbers\n",
      "Trained models on Food & Drink, added KNN to list\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Travel & Transportation due to low numbers\n",
      "Trained models on Health, added MLP to list\n",
      "Skipped category: Online Communities due to low numbers\n",
      "Trained models on News, added SGD to list\n",
      "Skipped category: Science due to low numbers\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Trained models on Law & Government, added MLP to list\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n"
     ]
    }
   ],
   "source": [
    "for size in size_list:\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(f\"Weibo Models with 0.2 confidence and {size} size\\n\\n\")\n",
    "\n",
    "    pheme_models, all_pheme_models = train_models(\"weibo\", weibo, 0.2, size, model_list_v2)\n",
    "    with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(\"Weibo\")\n",
    "        for key, value in pheme_models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained models on People & Society, added SVM to list\n",
      "Trained models on Arts & Entertainment, added MLP to list\n",
      "Trained models on Law & Government, added KNN to list\n",
      "Trained models on News, added KNN to list\n",
      "Skipped category: Food & Drink due to low numbers\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Trained models on Online Communities, added KNN to list\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Health due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Reference due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n",
      "Skipped category: Science due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Travel & Transportation due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Trained models on Arts & Entertainment, added SVM to list\n",
      "Trained models on People & Society, added KNN to list\n",
      "Skipped category: Reference due to low numbers\n",
      "Trained models on Food & Drink, added KNN to list\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Trained models on Travel & Transportation, added KNN to list\n",
      "Trained models on Health, added KNN to list\n",
      "Skipped category: Online Communities due to low numbers\n",
      "Trained models on News, added SVM to list\n",
      "Skipped category: Science due to low numbers\n",
      "Trained models on Sensitive Subjects, added KNN to list\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Trained models on Law & Government, added Logistic Regression to list\n",
      "Trained models on Business & Industrial, added KNN to list\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Trained models on Autos & Vehicles, added KNN to list\n"
     ]
    }
   ],
   "source": [
    "with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "    f.write(f\"Models with 0.2 confidence and 150 size\\n\\n\")\n",
    "\n",
    "twitter_models, all_twitter_models = train_models(\"twitter\", twitter_t, 0.2, 150, model_list_v2)\n",
    "with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(\"Twitter\\n\")\n",
    "        for key, value in twitter_models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "weibo_models, all_weibo_models = train_models(\"weibo\", weibo_t, 0.2, 150, model_list_v2)\n",
    "with open(\"model_parameters_by_c.txt\", \"a\") as f:\n",
    "        f.write(\"Weibo\\n\")\n",
    "        for key, value in weibo_models.items():\n",
    "            f.write(f\"(\\\"{key}\\\", \\\"{value[1]}\\\", {value[2]}),\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 185/200 [01:12<00:05,  2.57trial/s, best loss: -0.7750677506775068]\n",
      "Error training Logistic Regression in category Sensitive Subjects, skipping\n",
      "  4%|▍         | 8/200 [00:31<17:02,  5.32s/trial, best loss: -0.7249322493224932]"
     ]
    }
   ],
   "source": [
    "pheme_models1, all_pheme_models1 = train_models(\"pheme\", pheme_train, 0.2, 200, model_list_v2)\n",
    "pheme_models2, all_pheme_models2 = train_models(\"pheme\", pheme_train, 0.5, 5, model_list_v2)\n",
    "pheme_models4, all_pheme_models4 = train_models(\"pheme\", pheme_train, 0.3, 50, model_list_v2)\n",
    "\n",
    "a = [all_pheme_models1, all_pheme_models2, all_pheme_models3, all_pheme_models4]\n",
    "for k in a:\n",
    "    with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        f.write(\"\\nPHEME full model set\\n\")\n",
    "        for key, value in k.items():\n",
    "            f.write(f\"(\\\"{key}\\\", {value}),\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_models1, all_twitter_models1 = train_models(\"twitter\", twitter_t, 0.2, 200, model_list_v2)\n",
    "twitter_models2, all_twitter_models2 = train_models(\"twitter\", twitter_t, 0.5, 5, model_list_v2)\n",
    "twitter_models4, all_twitter_models4 = train_models(\"twitter\", twitter_t, 0.3, 50, model_list_v2)\n",
    "\n",
    "a = [all_twitter_models1, all_twitter_models2, all_twitter_models3, all_twitter_models4]\n",
    "for k in a:\n",
    "    with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTwitter full model set\\n\")\n",
    "        for key, value in k.items():\n",
    "            f.write(f\"(\\\"{key}\\\", {value}),\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_models1, all_weibo_models1 = train_models(\"weibo\", weibo_t, 0.2, 200, model_list_v2)\n",
    "weibo_models2, all_weibo_models2 = train_models(\"weibo\", weibo_t, 0.5, 5, model_list_v2)\n",
    "weibo_models4, all_weibo_models4 = train_models(\"weibo\", weibo_t, 0.3, 50, model_list_v2)\n",
    "\n",
    "a = [all_weibo_models1, all_weibo_models2, all_weibo_models3, all_weibo_models4]\n",
    "for k in a:\n",
    "    with open(\"optimized_model_parameters.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTwitter full model set\\n\")\n",
    "        for key, value in k.items():\n",
    "            f.write(f\"(\\\"{key}\\\", {value}),\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumour-ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
