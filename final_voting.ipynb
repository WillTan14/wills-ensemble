{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please please please be the final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "from hpsklearn import svc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial dataset stuff\n",
    "nlp = spacy.load(\"spacy-twitter\") # out of function so you don't load it every time (it takes a while)\n",
    "\n",
    "# function for glove embeddings\n",
    "def embed_dataset(dataset_text):\n",
    "    encoded = np.array([nlp(text).vector for text in dataset_text])\n",
    "    return encoded.tolist()\n",
    "\n",
    "# function to load dataset from folder. Also embeds the text.\n",
    "def get_dataset(name):\n",
    "    \"\"\"\n",
    "    loads a dataset and embeds the text. text must be in column named \"text\".\n",
    "    datasets are in the folder datasets/\n",
    "    name must be a string that's matches the csv file in datasets\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(f'datasets\\\\{name}.csv')\n",
    "    dataset.rename(columns = {\"Unnamed: 0\":\"entry\"}, inplace=True) #the entry label never carries over\n",
    "    dataset['e_text'] = embed_dataset(dataset['text'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator\n",
    "\n",
    "#function to optimize model with hyperopt\n",
    "def optimize_model(model, X_train, y_train, trials):\n",
    "    \"\"\"\n",
    "    optimizes a model with hyperopt. returns the optimized and fitted model.\n",
    "    \n",
    "    model: should match a sklearn model - list on github. can preset variables or set a range with hp\n",
    "    X_train and y_train: training set\n",
    "    trials: number of trials to do\n",
    "    \"\"\"\n",
    "    mod = HyperoptEstimator(classifier=model,\n",
    "                            preprocessing=[],\n",
    "                            max_evals=trials,\n",
    "                            trial_timeout=120,\n",
    "                            verbose=False)\n",
    "    mod.fit(X_train, y_train, random_state=42)\n",
    "    #print(mod.best_model())\n",
    "    return mod\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred_y = model.predict(X_test)\n",
    "    acc_mod = accuracy_score(y_test, pred_y)\n",
    "    print(\"Accuracy:\", float(\"{0:.2f}\".format(acc_mod*100)), \"%\")\n",
    "    f1_mod = f1_score(y_test, pred_y, average=\"macro\")\n",
    "    print(\"F1:\", float(\"{0:.2f}\".format(f1_mod*100)), \"%\")\n",
    "    cm = confusion_matrix(y_test, pred_y)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"true\", \"false\"])\n",
    "    disp.plot()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get breakdown of categories with confidence level, as well as dataset coverage\n",
    "def get_category_info(dataset_name, dataset, confidence, size_limit):\n",
    "    file_name = f\"{dataset_name}_cats\\\\{dataset_name}_categories_organised.json\"\n",
    "    f = open(file_name)\n",
    "    data = json.load(f)\n",
    "    info = []\n",
    "    dataset_coverage = np.zeros([len(dataset)], dtype=bool)\n",
    "    for category in data.keys():\n",
    "        cat_entries = [int(i) for i in data[category].keys() if data[category][i] > confidence]\n",
    "        if len(cat_entries) < size_limit:\n",
    "            print(f\"Skipped category: {category} due to low numbers\")\n",
    "            continue\n",
    "        info.append((category, len(cat_entries)))\n",
    "        for entry in cat_entries:\n",
    "            if dataset_coverage[entry] == False:\n",
    "                dataset_coverage[entry] = True\n",
    "    uniques, counts = np.unique(dataset_coverage, return_counts=True)\n",
    "    percentages = dict(zip(uniques, counts * 100 / len(dataset_coverage)))\n",
    "    return info, percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(dataset_name, train_set, confidence, size_limit, model_list):\n",
    "    \"\"\"\n",
    "    trains a set of models in each category. returns the best model for each category, in the form {'category': [modelscore, modelname, fittedmodel]}\n",
    "\n",
    "    dataset_name: a string with the name of the training set. used for calling the category file\n",
    "    train_set: the training set to use\n",
    "    confidence: the confidence required to consider an entry part of a category\n",
    "    size_limit: the number of entries needed in a category to consider that category for training\n",
    "    model_list: the list of models to train. in the form [(\"model_name1\", model1), (\"model_name2\", model2), etc]\n",
    "    \"\"\"\n",
    "    file_name = f\"{dataset_name}_cats\\\\{dataset_name}_categories_organised.json\"\n",
    "    f = open(file_name)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    category_models = {} #this will be returned\n",
    "    for category in data.keys(): \n",
    "        cat_entries = [int(i) for i in data[category].keys() if data[category][i] > confidence]\n",
    "        \n",
    "        # skip category if size of category is below limit\n",
    "        if len(cat_entries) < size_limit:\n",
    "            print(f\"Skipped category: {category} due to low numbers\")\n",
    "            continue\n",
    "        \n",
    "        category_data = train_set.filter(axis=0, items=cat_entries)\n",
    "\n",
    "        #split validation set\n",
    "        X = category_data.drop('target', axis=1)\n",
    "        y = category_data[\"target\"]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "\n",
    "        # skip category if split only has one class\n",
    "        if (len(np.unique(y_train)) <= 1):\n",
    "            print(f\"Skipped category: {category} due to class issues\")\n",
    "            continue\n",
    "\n",
    "        X_train_text = np.array([text for text in X_train['e_text']])\n",
    "\n",
    "        trained_models = []\n",
    "        # train models from list\n",
    "        for model_name, model in model_list:\n",
    "            try:\n",
    "                optimized = optimize_model(model, X_train_text, y_train, 1)\n",
    "                best_model = optimized.best_model()['learner'].fit(X_train_text, y_train)\n",
    "                trained_models.append((model_name, best_model))\n",
    "            except:\n",
    "                print(f\"Error training {model_name} in category {category}, skipping\")\n",
    "                continue\n",
    "        \n",
    "        #get the best model\n",
    "        X_val_text = np.array([text for text in X_val['e_text']])\n",
    "        best_model = [0, \"x\", \"x\"]\n",
    "        for model_name, model in trained_models:\n",
    "            score = model.score(X_val_text, y_val)\n",
    "            if score > best_model[0]:\n",
    "                best_model = [score, model_name, model]\n",
    "\n",
    "        #add best model to list\n",
    "        category_models[category] = best_model\n",
    "    return category_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_points(trained_models, test_cat_file, X_test):\n",
    "    \"\"\"\n",
    "    predict points using the trained models. returns an array of the predictions\n",
    "\n",
    "    trained_models: the models trained in each category, in the form {category: [modelscore, modelname, fittedmodel]}\n",
    "    test_cat_file: the filepath to the organised category file\n",
    "    X_test: the test set X values\n",
    "    \"\"\"\n",
    "    #load category data\n",
    "    f = open(test_cat_file)\n",
    "    category_data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # return arrays\n",
    "    final_predictions = []\n",
    "    \n",
    "    #embedded_text = np.array([text for text in X_test['e_text']])\n",
    "    for index, row in X_test.iterrows():\n",
    "        test_point = np.array([row['e_text']])\n",
    "        point_categories = category_data[str(index)]\n",
    "\n",
    "        # get weights of each point's topic\n",
    "        topic_weights = {}\n",
    "        for category in point_categories:\n",
    "            main_category = category.split(\"/\")[1]\n",
    "            if main_category not in trained_models.keys():\n",
    "                continue\n",
    "            if main_category in topic_weights:\n",
    "                topic_weights[main_category] += point_categories[category]\n",
    "            else:\n",
    "                topic_weights[main_category] = point_categories[category]\n",
    "        \n",
    "        #make topic predictions\n",
    "        model_predictions = []\n",
    "        for category in topic_weights.keys():\n",
    "            modelscore, modelname, model = trained_models[category]\n",
    "            prediction = model.predict(test_point)\n",
    "            model_predictions.append((prediction[0], modelscore, modelname, category))\n",
    "\n",
    "        # aggregate predictions\n",
    "        truefalse_scores = {True: 0, False:0}\n",
    "        for prediction, modelscore, modelname, category in model_predictions:\n",
    "            truefalse_scores[prediction] += modelscore\n",
    "        \n",
    "        #determine final prediction\n",
    "        if truefalse_scores[True] > truefalse_scores[False]:\n",
    "            final_predictions.append(True)\n",
    "        else:\n",
    "            final_predictions.append(False)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_score(test, pred):\n",
    "    acc = accuracy_score(test, pred)\n",
    "    f1 = f1_score(test, pred, average=\"macro\")\n",
    "    #cm = confusion_matrix(test, pred)\n",
    "    #disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"true\", \"false\"])\n",
    "    #disp.plot()\n",
    "    plt.show() \n",
    "    return float(\"{0:.2f}\".format(acc*100)), float(\"{0:.2f}\".format(f1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get dataset -> train models on dataset -> make predictions and vote -> evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_set, test_set, confidence, size_limit, model_list):\n",
    "    \"\"\"\n",
    "    Train and evaluate dataset on test set. Returns results and models\n",
    "    train_set: training set. In the form [\"dataset_name\", file_reference_name, dataset]\n",
    "    test_set: testing set(s). In the form [(\"dataset_name1\", file_reference_name1, dataset1), (\"dataset_name2\", file_reference_name2, dataset2), etc]\n",
    "    confidence: confidence threshold to consider a training point\n",
    "    size_limit: size threshold to consider training a category\n",
    "    model_list: list of models to train, in the form [(\"model_name1\", model1), (\"model_name2\", model2), etc]\n",
    "    \"\"\"\n",
    "    #prep dataset\n",
    "    dataset_name = train_set[0]\n",
    "    ref_name = train_set[1]\n",
    "    dataset = train_set[2]\n",
    "    X = dataset.drop(\"target\", axis=1)\n",
    "    y = dataset[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "    training_set = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    #train models\n",
    "    trained_models = train_models(ref_name, training_set, confidence, size_limit, model_list)\n",
    "\n",
    "    #make predictions on test set\n",
    "    test_cat_file = f\"{ref_name}_categories.json\"\n",
    "    predictions = predict_points(trained_models, test_cat_file, X_test)\n",
    "    results = check_score(y_test, predictions)\n",
    "\n",
    "    final_results = []\n",
    "    final_results.append((dataset_name, results))\n",
    "\n",
    "    #make predictions on out of domain test sets\n",
    "    for set_name, ref_name, set_data in test_set:\n",
    "        test_cat_file = f\"{ref_name}_categories.json\"\n",
    "        X = set_data.drop(\"target\", axis=1)\n",
    "        y = set_data[\"target\"]\n",
    "        predictions = predict_points(trained_models, test_cat_file, set_data)\n",
    "        results = check_score(y, predictions)\n",
    "        final_results.append((set_name, results))\n",
    "    \n",
    "    return trained_models, final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(tests, confidence, size_limit, model_list):\n",
    "    test_results = []\n",
    "    trained_models = []\n",
    "    for i in tqdm(range(len(tests))):\n",
    "        t = tests.copy()\n",
    "        train = t.pop(i)\n",
    "        models, results = train_and_evaluate(train, t, confidence, size_limit, model_list)\n",
    "        test_results.append((train[0], results))\n",
    "        trained_models.append((train[0], models))\n",
    "    return trained_models, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [13:46<18:53, 103.05s/trial, best loss: -0.8140077821011673]"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pheme = get_dataset(\"pheme\")\n",
    "X = pheme.drop(\"target\", axis=1)\n",
    "y = pheme[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "X_train_text = np.array([text for text in X_train['e_text']])\n",
    "X_test_text = np.array([text for text in X_test['e_text']])\n",
    "\n",
    "def objective(n_estimators):\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                   max_features='sqrt',\n",
    "                                   random_state=42)\n",
    "    model.fit(X_train_text, y_train)\n",
    "    y_pred = model.predict(X_test_text)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "search_space = hp.randint('n_estimators', 200, 1000)\n",
    "algorithm=tpe.suggest\n",
    "best_params = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algorithm,\n",
    "  max_evals=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import svc, k_neighbors_classifier, logistic_regression\n",
    "model_list = [\n",
    "    (\"SVC\", svc(\"SVC\", random_state=42, probability=True)),\n",
    "    (\"KNN\", k_neighbors_classifier(\"knn\")),\n",
    "    (\"Logistic Regression\", logistic_regression(\"LR\", random_state=42, solver=\"saga\", penalty=hp.choice(\"penalty\", {None, \"l1\", \"l2\"}))),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme = get_dataset(\"pheme\")\n",
    "twitter = get_dataset(\"twitter\")\n",
    "twitter15 = twitter.iloc[:1491]\n",
    "twitter16 = twitter.iloc[1491:]\n",
    "weibo = get_dataset(\"weibo\")\n",
    "weibo = weibo.drop([1933, 3564])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [[\"PHEME\", \"pheme\", pheme], [\"twitter15\", \"twitter\", twitter15], [\"twitter16\", \"twitter\", twitter16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests2 = [[\"PHEME\", \"pheme\", pheme], [\"twitterFULL\", \"twitter\", twitter], [\"WEIBO\", \"weibo\", weibo]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.97s/trial, best loss: 0.23050847457627122]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.92s/trial, best loss: 0.22372881355932206]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.02s/trial, best loss: 0.24745762711864405]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.28s/trial, best loss: 0.4148606811145511]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/trial, best loss: 0.28792569659442724]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.42s/trial, best loss: 0.2662538699690402]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/trial, best loss: 0.23703703703703705]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.19999999999999996]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.86s/trial, best loss: 0.20740740740740737]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/trial, best loss: 0.128]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.83s/trial, best loss: 0.11199999999999999]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/trial, best loss: 0.11199999999999999]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.03s/trial, best loss: 0.28402366863905326]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/trial, best loss: 0.3195266272189349]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/trial, best loss: 0.28402366863905326]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/trial, best loss: 0.17142857142857137]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/trial, best loss: 0.17142857142857137]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.11s/trial, best loss: 0.1785714285714286]\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Reference due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Skipped category: Health due to low numbers\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n",
      "Skipped category: Food & Drink due to low numbers\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.20512820512820518]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.79s/trial, best loss: 0.17948717948717952]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.79s/trial, best loss: 0.2564102564102564]\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Science due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Finance due to low numbers\n"
     ]
    }
   ],
   "source": [
    "models, results = train_and_evaluate([\"PHEME\", \"pheme\", pheme], [[\"twitter15\", \"twitter\", twitter15], [\"twitter16\", \"twitter\", twitter16]], 0.2, 200, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.87s/trial, best loss: 0.29491525423728815]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.07s/trial, best loss: 0.21525423728813564]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.80s/trial, best loss: 0.25254237288135595]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.19s/trial, best loss: 0.2538699690402477]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.80s/trial, best loss: 0.23529411764705888]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.29s/trial, best loss: 0.24148606811145512]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/trial, best loss: 0.2148148148148148]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.71s/trial, best loss: 0.19259259259259254]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.20740740740740737]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/trial, best loss: 0.128]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.71s/trial, best loss: 0.128]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.87s/trial, best loss: 0.10399999999999998]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.29s/trial, best loss: 0.3076923076923077]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/trial, best loss: 0.3254437869822485]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.94s/trial, best loss: 0.28402366863905326]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/trial, best loss: 0.16428571428571426]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.18571428571428572]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/trial, best loss: 0.1785714285714286]\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Reference due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Skipped category: Health due to low numbers\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n",
      "Skipped category: Food & Drink due to low numbers\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/trial, best loss: 0.2564102564102564]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/trial, best loss: 0.17948717948717952]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/trial, best loss: 0.28205128205128205]\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Science due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Finance due to low numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:20<02:41, 80.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.71s/trial, best loss: 0.2954545454545454]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.87s/trial, best loss: 0.2954545454545454]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.79s/trial, best loss: 0.2727272727272727]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/trial, best loss: 0.22033898305084743]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.3389830508474576]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/trial, best loss: 0.23728813559322037]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.27118644067796616]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/trial, best loss: 0.11864406779661019]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.68s/trial, best loss: 0.27118644067796616]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.79s/trial, best loss: 0.4285714285714286]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/trial, best loss: 0.2941176470588235]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/trial, best loss: 0.2773109243697479]\n",
      "Skipped category: Food & Drink due to low numbers\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.84s/trial, best loss: 0.25]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/trial, best loss: 0.18548387096774188]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/trial, best loss: 0.29032258064516125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.86s/trial, best loss: 0.6666666666666667]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.12s/trial, best loss: 0.36111111111111116]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.02s/trial, best loss: 0.5]\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Health due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Reference due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n",
      "Skipped category: Science due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Travel & Transportation due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [02:28<01:13, 73.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.76s/trial, best loss: 0.38181818181818183]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.71s/trial, best loss: 0.2545454545454545]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.89s/trial, best loss: 0.1636363636363637]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.77s/trial, best loss: 0.4558823529411765]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/trial, best loss: 0.19117647058823528]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/trial, best loss: 0.13235294117647056]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped category: Reference due to low numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/trial, best loss: 0.25806451612903225]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.84s/trial, best loss: 0.16129032258064513]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/trial, best loss: 0.19354838709677424]\n",
      "Skipped category: Sports due to low numbers\n",
      "Skipped category: Games due to low numbers\n",
      "Skipped category: Travel & Transportation due to low numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.2666666666666667]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/trial, best loss: 0.09999999999999998]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/trial, best loss: 0.16666666666666663]\n",
      "Skipped category: Online Communities due to low numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.71s/trial, best loss: 0.41666666666666663]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/trial, best loss: 0.18055555555555558]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.83s/trial, best loss: 0.19444444444444442]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped category: Science due to low numbers\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willc\\miniconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/trial, best loss: 0.21621621621621623]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/trial, best loss: 0.14414414414414412]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.76s/trial, best loss: 0.2792792792792793]\n",
      "Skipped category: Shopping due to low numbers\n",
      "Skipped category: Finance due to low numbers\n",
      "Skipped category: Real Estate due to low numbers\n",
      "Skipped category: Jobs & Education due to low numbers\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/trial, best loss: 0.2142857142857143]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/trial, best loss: 0.34285714285714286]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.77s/trial, best loss: 0.24285714285714288]\n",
      "Skipped category: Business & Industrial due to low numbers\n",
      "Skipped category: Computers & Electronics due to low numbers\n",
      "Skipped category: Internet & Telecom due to low numbers\n",
      "Skipped category: Hobbies & Leisure due to low numbers\n",
      "Skipped category: Books & Literature due to low numbers\n",
      "Skipped category: Beauty & Fitness due to low numbers\n",
      "Skipped category: Pets & Animals due to low numbers\n",
      "Skipped category: Adult due to low numbers\n",
      "Skipped category: Home & Garden due to low numbers\n",
      "Skipped category: Autos & Vehicles due to low numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:13<00:00, 64.64s/it]\n"
     ]
    }
   ],
   "source": [
    "models, results = run_tests(tests2, 0.2, 200, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PHEME',\n",
       "  [('PHEME', (81.4, 79.98)),\n",
       "   ('twitterFULL', (55.24, 52.99)),\n",
       "   ('WEIBO', (50.0, 37.38))]),\n",
       " ('twitterFULL',\n",
       "  [('twitterFULL', (73.59, 73.56)),\n",
       "   ('PHEME', (54.72, 54.52)),\n",
       "   ('WEIBO', (50.39, 50.35))]),\n",
       " ('WEIBO',\n",
       "  [('WEIBO', (73.74, 73.55)),\n",
       "   ('PHEME', (51.11, 48.08)),\n",
       "   ('twitterFULL', (53.55, 53.29))])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organise_results(list_of_results):\n",
    "    for results in list_of_results:\n",
    "        for training_set, metrics in results:\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
